
@book{wieringa_design_2014,
	address = {Berlin, Heidelberg},
	title = {Design {Science} {Methodology} for {Information} {Systems} and {Software} {Engineering}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-43838-1 978-3-662-43839-8},
	url = {https://link.springer.com/10.1007/978-3-662-43839-8},
	doi = {10.1007/978-3-662-43839-8},
	language = {en},
	urldate = {2026-02-04},
	publisher = {Springer Berlin Heidelberg},
	author = {Wieringa, Roel J.},
	year = {2014},
	doi = {10.1007/978-3-662-43839-8},
}

@article{seo_shift_2026,
	title = {{SHIFT}: {Self}-{Healing} {Intelligence} in {Feature} store and data store {Transitions}},
	issn = {2169-3536},
	shorttitle = {{SHIFT}},
	url = {https://ieeexplore.ieee.org/abstract/document/11366656},
	doi = {10.1109/ACCESS.2026.3658719},
	abstract = {This study presents an AI model management method for maintaining the predictive performance of virtual metrology (VM) systems in steel manufacturing processes. The proposed method addresses concept drift (CD), a major threat to long-term model reliability, by orchestrating a dual-update strategy that autonomously revises both the feature store and the data store. Unlike conventional model maintenance approaches based on static inputs and accumulated data, our framework continuously reassesses feature relevance and adapts retraining datasets to reflect evolving process conditions. Applied to a real-world steel sintering process, the method achieved significant recovery in predictive accuracy—from a post-deployment drop of 65\% to over 93\% after updates. This study highlights the importance of proactive maintenance for AI models and demonstrates how an intelligent update strategy can extend the usable life and reliability of VM systems in critical production workflows. The findings contribute to the broader field of AI-enabled asset management by offering a scalable solution for condition monitoring, retraining scheduling, and long-term performance assurance.},
	urldate = {2026-02-04},
	journal = {IEEE Access},
	author = {Seo, Seong-Hyun and Lim, Dong-Joon},
	year = {2026},
	keywords = {Adaptation models, Artificial intelligence, Concept drift, Data models, Feature extraction, Maintenance, Monitoring, Operational intelligence, Predictive models, Reliability, Steel, autonomous model update, concept drift, data store, feature store, virtual metrology},
	pages = {1--1},
}

@misc{qi_datacross_2026,
	title = {{DataCross}: {A} {Unified} {Benchmark} and {Agent} {Framework} for {Cross}-{Modal} {Heterogeneous} {Data} {Analysis}},
	shorttitle = {{DataCross}},
	url = {http://arxiv.org/abs/2601.21403},
	doi = {10.48550/arXiv.2601.21403},
	abstract = {In real-world data science and enterprise decisionmaking, critical information is often fragmented across directly queryable structured sources (e.g., SQL, CSV) and “zombie data” locked in unstructured visual documents (e.g., scanned reports, invoice images). Existing data analytics agents are predominantly limited to processing structured data, failing to activate and correlate this highvalue visual information, thus creating a significant gap with industrial needs. To bridge this gap, we introduce DataCross, a novel benchmark and collaborative agent framework for unified, insightdriven analysis across heterogeneous data modalities. DataCrossBench comprises 200 end-to-end analysis tasks across finance, healthcare, and other domains. It is constructed via a human-in-theloop reverse-synthesis pipeline, ensuring realistic complexity, cross-source dependency, and verifiable ground truth. The benchmark categorizes tasks into three difficulty tiers to evaluate agents’ capabilities in visual table extraction, cross-modal alignment, and multi-step joint reasoning.We also propose the DataCrossAgent framework, inspired by the “divide-and-conquer” workflow of human analysts. It employs specialized sub-agents, each an expert on a specific data source, which are coordinated via a structured workflow of Intra-source Deep Exploration, Key Source Identification, and Contextual Cross-pollination. A novel reReAct mechanism enables robust code generation and debugging for factual verification. Experimental results show that DataCrossAgent achieves a 29.7\% improvement in factuality over GPT-4o and exhibits superior robustness on high-difficulty tasks, effectively activating fragmented “zombie data” for insightful, cross-modal analysis.},
	language = {en},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Qi, Ruyi and Liu, Zhou and Zhang, Wentao},
	month = jan,
	year = {2026},
	note = {arXiv:2601.21403 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
}

@article{yang_zero-training_2026,
	title = {A {Zero}-{Training} {Data} {Cleaning} {System} {With} {Large} {Language} {Models}},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/abstract/document/11367302},
	doi = {10.1109/TKDE.2026.3658627},
	abstract = {Data cleaning (DC) is a crucial yet challenging step for many data engineering tasks. Traditional pre-configuration DC methods rely heavily on predefined rules or constraints, demanding significant domain knowledge and manual effort. While configuration-free DC approaches have been explored, they still demand extensive feature engineering or labeled data for intensive model training. In this paper, we propose a zero-training and interpretable DC system, named {\textbackslash}sf ZeroDC, that leverages large language models (LLMs) to generate data cleaning rules and chain-of-thoughts (CoTs), without the need for model training. {\textbackslash}sf ZeroDC consists of two modules, iterative detection rule generation (IDG) and training-free explainable correction (TEC). To generate high-quality error detection rules with minimal human feedback, IDG first bootstraps a set of rules via contrastive rule initiation on sampled syntactic and semantic contrastive pairs. It then progressively enhances them through an iterative rule refinement workflow that selects the most informative elements for updates. TEC constructs a contextual-relevant tuple retriever using a weighted cosine similarity function to efficiently identify the most relevant tuples for each dirty value, reducing redundancy in the LLM prompts and lowering computational costs. It further prompts for generating correction CoTs for user-corrected representative values, as well as prompts for creating correction rules and explainable corrections, which automatically provide explanations for correction results, all without the need for model training. Extensive experiments conducted on various real-world datasets demonstrate that {\textbackslash}sf ZeroDC achieves, on average, a 5.36\% increase in accuracy and an 8.16x speedup compared to state-of-the-art methods. The codes and datasets of this paper are available at https://github.com/YangChen32768/ZeroDC.},
	urldate = {2026-02-04},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Yang, Chen and Huang, Kai and Wu, Yue and Wu, Yangyang and Zhu, Mengying and Miao, Xiaoye and Xi, Meng and Zheng, Xiaolin and Yin, Jianwei},
	year = {2026},
	keywords = {Annotations, Cleaning, Cleaning rule generation, Data models, Error correction, Iterative methods, Large language models, Machine learning, Semantics, Syntactics, Training, correction chain-of-thoughts, data cleaning, large language models},
	pages = {1--14},
}

@misc{li_human-llm_2026,
	title = {Human-{LLM} {Collaborative} {Feature} {Engineering} for {Tabular} {Data}},
	url = {http://arxiv.org/abs/2601.21060},
	doi = {10.48550/arXiv.2601.21060},
	abstract = {Large language models (LLMs) are increasingly used to automate feature engineering in tabular learning. Given task-specific information, LLMs can propose diverse feature transformation operations to enhance downstream model performance. However, current approaches typically assign the LLM as a black-box optimizer, responsible for both proposing and selecting operations based solely on its internal heuristics, which often lack calibrated estimations of operation utility and consequently lead to repeated exploration of low-yield operations without a principled strategy for prioritizing promising directions. In this paper, we propose a human-LLM collaborative feature engineering framework for tabular learning. We begin by decoupling the transformation operation proposal and selection processes, where LLMs are used solely to generate operation candidates, while the selection is guided by explicitly modeling the utility and uncertainty of each proposed operation. Since accurate utility estimation can be difficult especially in the early rounds of feature engineering, we design a mechanism within the framework that selectively elicits and incorporates human expert preference feedback, comparing which operations are more promising, into the selection process to help identify more effective operations. Our evaluations on both the synthetic study and the real user study demonstrate that the proposed framework improves feature engineering performance across a variety of tabular datasets and reduces users' cognitive load during the feature engineering process.},
	urldate = {2026-02-04},
	publisher = {arXiv},
	author = {Li, Zhuoyan and Bansal, Aditya and Li, Jinzhao and He, Shishuang and Lu, Zhuoran and Zhang, Mutian and Liu, Qin and Yang, Yiwei and Jain, Swati and Yin, Ming and Li, Yunyao},
	month = jan,
	year = {2026},
	note = {arXiv:2601.21060 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{noauthor_openaiskills_nodate,
	title = {openai/skills: {Skills} {Catalog} for {Codex}},
	url = {https://github.com/openai/skills},
	urldate = {2026-02-04},
}

@misc{noauthor_masonclreview-prompts_nodate,
	title = {masoncl/review-prompts: {AI} review prompts},
	url = {https://github.com/masoncl/review-prompts},
	urldate = {2026-02-04},
}

@misc{noauthor_qwen_nodate,
	title = {Qwen},
	url = {https://qwen.ai/blog?id=qwen3-coder-next},
	urldate = {2026-02-04},
}

@misc{jovanovich_sub-agents_2025,
	type = {Substack newsletter},
	title = {Sub-{Agents} in {Claude} {Code}: {The} {Subagent} {Orchestration} {Behind} the {Response} {Awareness} {Methodology}},
	shorttitle = {Sub-{Agents} in {Claude} {Code}},
	url = {https://responseawareness.substack.com/p/sub-agents-in-claude-code-the-subagent},
	abstract = {From beginner to expert},
	urldate = {2026-02-03},
	journal = {Claude Code: Response Awareness Methodology},
	author = {Jovanovich, Michael},
	month = sep,
	year = {2025},
}

@misc{noauthor_fission-aiopenspec_2026,
	title = {Fission-{AI}/{OpenSpec}},
	copyright = {MIT},
	url = {https://github.com/Fission-AI/OpenSpec},
	abstract = {Spec-driven development (SDD) for AI coding assistants.},
	urldate = {2026-02-03},
	publisher = {Fission},
	month = feb,
	year = {2026},
	note = {original-date: 2025-08-05T10:37:45Z},
	keywords = {ai, context-engineering, engineering, planning, prd, sdd, sdlc, spec, spec-driven-development, specification},
}

@misc{rbby_github_2026,
	title = {Github {Browser} {Plugin} for {Ai} {Contribution} {Blame} in {Pull} {Requests}},
	url = {https://blog.rbby.dev/posts/github-ai-contribution-blame-for-pull-requests/},
	abstract = {An extension of Refined GitHub browser plugin for AI contribution tracking capabilities in GitHub pull requests.},
	language = {en},
	urldate = {2026-02-03},
	journal = {rbby.dev},
	author = {rbby},
	month = jan,
	year = {2026},
	note = {Section: posts},
}

@misc{noauthor_cursor_2026,
	title = {Cursor},
	url = {https://cursor.com},
	abstract = {Built to make you extraordinarily productive, Cursor is the best way to code with AI.},
	language = {en-US},
	urldate = {2026-02-03},
	journal = {Cursor},
	month = jan,
	year = {2026},
}

@misc{noauthor_agentskillsagentskills_2026,
	title = {agentskills/agentskills},
	url = {https://github.com/agentskills/agentskills},
	abstract = {Specification and documentation for Agent Skills},
	urldate = {2026-02-03},
	publisher = {Agent Skills},
	month = feb,
	year = {2026},
	note = {original-date: 2025-12-16T15:47:19Z},
	keywords = {agent-skills},
}

@misc{noauthor_obrasuperpowers_nodate,
	title = {obra/superpowers: {An} agentic skills framework \& software development methodology that works.},
	url = {https://github.com/obra/superpowers},
	urldate = {2026-02-03},
}

@misc{noauthor_vm0-aivm0_nodate,
	title = {vm0-ai/vm0: the easiest way to run natural language-described workflows automatically},
	url = {https://github.com/vm0-ai/vm0?tab=readme-ov-file},
	urldate = {2026-02-03},
}

@misc{noauthor_obrasuperpowers_nodate-1,
	title = {obra/superpowers: {An} agentic skills framework \& software development methodology that works.},
	url = {https://github.com/obra/superpowers},
	urldate = {2026-02-03},
}

@misc{noauthor_thedotmackclaude-mem_nodate,
	title = {thedotmack/claude-mem: {A} {Claude} {Code} plugin that automatically captures everything {Claude} does during your coding sessions, compresses it with {AI} (using {Claude}'s agent-sdk), and injects relevant context back into future sessions.},
	url = {https://github.com/thedotmack/claude-mem},
	urldate = {2026-02-03},
}

@article{jiang_deepseek_2025,
	title = {{DeepSeek} vs. {ChatGPT} vs. {Claude}: {A} comparative study for scientific computing and scientific machine learning tasks},
	volume = {15},
	issn = {2095-0349},
	shorttitle = {{DeepSeek} vs. {ChatGPT} vs. {Claude}},
	url = {https://www.sciencedirect.com/science/article/pii/S2095034925000157},
	doi = {10.1016/j.taml.2025.100583},
	abstract = {Large language models (LLMs) have emerged as powerful tools for addressing a wide range of problems, including those in scientific computing, particularly in solving partial differential equations (PDEs). However, different models exhibit distinct strengths and preferences, resulting in varying levels of performance. In this paper, we compare the capabilities of the most advanced LLMs—DeepSeek, ChatGPT, and Claude—along with their reasoning-optimized versions in addressing computational challenges. Specifically, we evaluate their proficiency in solving traditional numerical problems in scientific computing as well as leveraging scientific machine learning techniques for PDE-based problems. We designed all our experiments so that a nontrivial decision is required, e.g, defining the proper space of input functions for neural operator learning. Our findings show that reasoning and hybrid-reasoning models consistently and significantly outperform non-reasoning ones in solving challenging problems, with ChatGPT o3-mini-high generally offering the fastest reasoning speed.},
	number = {3},
	urldate = {2026-02-03},
	journal = {Theoretical and Applied Mechanics Letters},
	author = {Jiang, Qile and Gao, Zhiwei and Karniadakis, George Em},
	month = may,
	year = {2025},
	keywords = {Large language models (LLM), Physics-informed neural network, Scientific computing, Scientific machine learning},
	pages = {100583},
}

@inproceedings{chatlatanagulchai_use_2026,
	address = {Cham},
	title = {On the {Use} of {Agentic} {Coding} {Manifests}: {An} {Empirical} {Study} of {Claude} {Code}},
	isbn = {978-3-032-12089-2},
	shorttitle = {On the {Use} of {Agentic} {Coding} {Manifests}},
	doi = {10.1007/978-3-032-12089-2_40},
	abstract = {Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write/execute the actual code with minimal human intervention. Key to this process are agent manifests, configuration files (such as Claude.md) that provide agents with essential project context, identity, and operational rules. However, the lack of comprehensive and accessible documentation for creating these manifests presents a significant challenge for developers. We analyzed 253 Claude.md files from 242 repositories to identify structural patterns and common content. Our findings show that manifests typically have shallow hierarchies with one main heading and several subsections, with content dominated by operational commands, technical implementation notes, and high-level architecture.},
	language = {en},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer Nature Switzerland},
	author = {Chatlatanagulchai, Worawalan and Thonglek, Kundjanasith and Reid, Brittany and Kashiwa, Yutaro and Leelaprute, Pattara and Rungsawang, Arnon and Manaskasemsak, Bundit and Iida, Hajimu},
	editor = {Scanniello, Giuseppe and Lenarduzzi, Valentina and Romano, Simone and Vegas, Sira and Francese, Rita},
	year = {2026},
	keywords = {Agentic Coding, Autonomous Programming, Documents},
	pages = {543--551},
}

@misc{mohsenimofidi_context_2026,
	title = {Context {Engineering} for {AI} {Agents} in {Open}-{Source} {Software}},
	url = {http://arxiv.org/abs/2510.21413},
	doi = {10.48550/arXiv.2510.21413},
	abstract = {GenAI-based coding assistants have disrupted software development. The next generation of these tools is agent-based, operating with more autonomy and potentially without human oversight. Like human developers, AI agents require contextual information to develop solutions that are in line with the standards, policies, and workflows of the software projects they operate in. Vendors of popular agentic tools (e.g., Claude Code) recommend maintaining version-controlled Markdown files that describe aspects such as the project structure, code style, or building and testing. The content of these files is then automatically added to each prompt. Recently, AGENTS.md has emerged as a potential standard that consolidates existing tool-specific formats. However, little is known about whether and how developers adopt this format. Therefore, in this paper, we present the results of a preliminary study investigating the adoption of AI context files in 466 open-source software projects. We analyze the information that developers provide in AGENTS.md files, how they present that information, and how the files evolve over time. Our findings indicate that there is no established content structure yet and that there is a lot of variation in terms of how context is provided (descriptive, prescriptive, prohibitive, explanatory, conditional). Our commit-level analysis provides first insights into the evolution of the provided context. AI context files provide a unique opportunity to study real-world context engineering. In particular, we see great potential in studying which structural or presentational modifications can positively affect the quality of the generated content.},
	urldate = {2026-02-03},
	publisher = {arXiv},
	author = {Mohsenimofidi, Seyedmoein and Galster, Matthias and Treude, Christoph and Baltes, Sebastian},
	month = jan,
	year = {2026},
	note = {arXiv:2510.21413 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@article{palla_evaluation_2025,
	title = {Evaluation of {Generative} {AI} {Models} in {Python} {Code} {Generation}: {A} {Comparative} {Study}},
	volume = {13},
	issn = {2169-3536},
	shorttitle = {Evaluation of {Generative} {AI} {Models} in {Python} {Code} {Generation}},
	url = {https://ieeexplore.ieee.org/abstract/document/10963975},
	doi = {10.1109/ACCESS.2025.3560244},
	abstract = {This study evaluates leading generative AI models for Python code generation. Evaluation criteria include syntax accuracy, response time, completeness, reliability, and cost. The models tested comprise OpenAI’s GPT series (GPT-4 Turbo, GPT-4o, GPT-4o Mini, GPT-3.5 Turbo), Google’s Gemini (1.0 Pro, 1.5 Flash, 1.5 Pro), Meta’s LLaMA (3.0 8B, 3.1 8B), and Anthropic’s Claude models (3.5 Sonnet, 3 Opus, 3 Sonnet, 3 Haiku). Ten coding tasks of varying complexity were tested across three iterations per model to measure performance and consistency. Claude models, especially Claude 3.5 Sonnet, achieved the highest accuracy and reliability. They outperformed all other models in both simple and complex tasks. Gemini models showed limitations in handling complex code. Cost-effective options like Claude 3 Haiku and Gemini 1.5 Flash were budget-friendly and maintained good accuracy on simpler problems. Unlike earlier single-metric studies, this work introduces a multi-dimensional evaluation framework that considers accuracy, reliability, cost, and exception handling. Future work will explore other programming languages and include metrics such as code optimization and security robustness.},
	urldate = {2026-02-03},
	journal = {IEEE Access},
	author = {Palla, Dominik and Slaby, Antonin},
	year = {2025},
	keywords = {Accuracy, Artificial intelligence, Automatization, Codes, Costs, Encoding, Generative AI, Internet, LLM, Python, Reliability, Software development management, generative AI, python, software development},
	pages = {65334--65347},
}

@misc{li_rise_2025,
	title = {The {Rise} of {AI} {Teammates} in {Software} {Engineering} ({SE}) 3.0: {How} {Autonomous} {Coding} {Agents} {Are} {Reshaping} {Software} {Engineering}},
	shorttitle = {The {Rise} of {AI} {Teammates} in {Software} {Engineering} ({SE}) 3.0},
	url = {http://arxiv.org/abs/2507.15003},
	doi = {10.48550/arXiv.2507.15003},
	abstract = {The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development. Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics). We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI\_Teammates\_in\_SE3. {\textgreater} AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent},
	urldate = {2026-02-03},
	publisher = {arXiv},
	author = {Li, Hao and Zhang, Haoxiang and Hassan, Ahmed E.},
	month = jul,
	year = {2025},
	note = {arXiv:2507.15003 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Engineering, Finance, and Science, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{chatlatanagulchai_agent_2025,
	title = {Agent {READMEs}: {An} {Empirical} {Study} of {Context} {Files} for {Agentic} {Coding}},
	shorttitle = {Agent {READMEs}},
	url = {http://arxiv.org/abs/2511.12884},
	doi = {10.48550/arXiv.2511.12884},
	abstract = {Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3\%), implementation details (69.9\%), and architecture (67.7\%). We also identify a significant gap: non-functional requirements like security (14.5\%) and performance (14.5\%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.},
	urldate = {2026-02-03},
	publisher = {arXiv},
	author = {Chatlatanagulchai, Worawalan and Li, Hao and Kashiwa, Yutaro and Reid, Brittany and Thonglek, Kundjanasith and Leelaprute, Pattara and Rungsawang, Arnon and Manaskasemsak, Bundit and Adams, Bram and Hassan, Ahmed E. and Iida, Hajimu},
	month = nov,
	year = {2025},
	note = {arXiv:2511.12884 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{team_memu_nodate,
	title = {{memU} {Bot} - {Your} {Personalized} {AI} {Assistant} {That} {Works} 24/7},
	url = {https://memu.bot/},
	abstract = {A hyper-personalized AI assistant that works around the clock. It learns your habits, understands your needs, and takes action for you — even while you sleep.},
	language = {English},
	urldate = {2026-02-03},
	journal = {memU bot},
	author = {Team, memU},
}

@misc{noauthor_hacking_2026,
	title = {Hacking {Moltbook}: {AI} {Social} {Network} {Reveals} 1.{5M} {API} {Keys} {\textbar} {Wiz} {Blog}},
	shorttitle = {Hacking {Moltbook}},
	url = {https://www.wiz.io/blog/exposed-moltbook-database-reveals-millions-of-api-keys},
	abstract = {Learn how a misconfigured Supabase database at Moltbook exposed 1.5M API keys, private messages, and user emails, enabling full AI agent takeover.},
	language = {en-us},
	urldate = {2026-02-03},
	journal = {wiz.io},
	month = feb,
	year = {2026},
}

@misc{noauthor_understanding_2026,
	title = {Understanding {LLM} {Inference} {Engines}: {Inside} {Nano}-{vLLM} ({Part} 1) - {Neutree} {Blog}},
	shorttitle = {Understanding {LLM} {Inference} {Engines}},
	url = {https://neutree.ai/blog/nano-vllm-part-1},
	abstract = {When deploying large language models in production, the inference engine becomes a critical piece of infrastructure.},
	language = {en},
	urldate = {2026-02-03},
	journal = {Neutree},
	month = feb,
	year = {2026},
}

@misc{noauthor_introducing_2026,
	title = {Introducing the {Codex} app},
	url = {https://openai.com/index/introducing-the-codex-app/},
	abstract = {Introducing the Codex app for macOS—a command center for AI coding and software development with multiple agents, parallel workflows, and long-running tasks.},
	language = {en-US},
	urldate = {2026-02-03},
	month = jan,
	year = {2026},
}

@misc{inference_labs_oogunbiyi21stats-compass-mcp_2026,
	title = {oogunbiyi21/stats-compass-mcp},
	copyright = {MIT},
	url = {https://github.com/oogunbiyi21/stats-compass-mcp},
	abstract = {Stats Compass MCP server and utils},
	urldate = {2026-02-03},
	author = {inference\_labs},
	month = feb,
	year = {2026},
	note = {original-date: 2025-12-11T08:49:00Z},
}

@article{hevner_design_2004,
	title = {Design science in information systems research},
	volume = {28},
	issn = {0276-7783},
	abstract = {Two paradigms characterize much of the research in the Information Systems discipline: behavioral science and design science. The behavioral-science paradigm seeks to develop and verify theories that explain or predict human or organizational behavior. The design-science paradigm seeks to extend the boundaries of human and organizational capabilities by creating new and innovative artifacts. Both paradigms are foundational to the IS discipline, positioned as it is at the confluence of people, organizations, and technology. Our objective is to describe the performance of design-science research in Information Systems via a concise conceptual framework and clear guidelines for understanding, executing, and evaluating the research. In the design-science paradigm, knowledge and understanding of a problem domain and its solution are achieved in the building and application of the designed artifact. Three recent exemplars in the research literature are used to demonstrate the application of these guidelines. We conclude with an analysis of the challenges of performing high-quality design-science research in the context of the broader IS community.},
	number = {1},
	journal = {MIS Quarterly},
	author = {Hevner, Alan R. and March, Salvatore T. and Park, Jinsoo and Ram, Sudha},
	month = mar,
	year = {2004},
	pages = {75--105},
}

@article{hevner_design_nodate,
	title = {Design {Science} in {Information} {Systems} {Research}},
	language = {en},
	author = {Hevner, Alan R and March, Salvatore T and Park, Jinsoo and Ram, Sudha},
}

@misc{noauthor_openlit_nodate,
	title = {{OpenLIT} {\textbar} {OpenTelemetry}-native {GenAI} and {LLM} {Application} {Observability}},
	url = {https://openlit.io},
	abstract = {Elevate APM with OpenLIT, the open-source platform built on OpenTelemetry. Simplify observability with unified traces and metrics in one powerful interface. Transform development effortlessly today!},
	language = {en-us},
	urldate = {2026-02-02},
	journal = {OpenLIT {\textbar} OpenTelemetry-native GenAI and LLM Application Observability},
}

@misc{noauthor_keda_nodate,
	title = {{KEDA}},
	url = {https://keda.sh/},
	abstract = {Application autoscaling made simple},
	language = {en-us},
	urldate = {2026-02-02},
	journal = {KEDA},
}

@misc{zhang_dynamic_2026,
	title = {Dynamic and {Adaptive} {Feature} {Generation} with {LLM}},
	url = {http://arxiv.org/abs/2406.03505},
	doi = {10.48550/arXiv.2406.03505},
	abstract = {The representation of feature space is a crucial environment where data points get vectorized and embedded for subsequent modeling. Thus the efficacy of machine learning (ML) algorithms is closely related to the quality of feature engineering. As one of the most important techniques, feature generation transforms raw data into an optimized feature space conducive to model training and further refines the space. Despite the advancements in automated feature engineering and feature generation, current methodologies often suffer from three fundamental issues: lack of explainability, limited applicability, and inflexible strategy. These shortcomings frequently hinder and limit the deployment of ML models across varied scenarios. Our research introduces a novel approach adopting large language models (LLMs) and feature-generating prompts to address these challenges. We propose a dynamic and adaptive feature generation method that enhances the interpretability of the feature generation process. Our approach broadens the applicability across various data types and tasks and offers advantages over strategic flexibility. A broad range of experiments showcases that our approach is significantly superior to existing methods.},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Zhang, Xinhao and Zhang, Jinghan and Rekabdar, Banafsheh and Zhou, Yuanchun and Wang, Pengfei and Liu, Kunpeng},
	month = jan,
	year = {2026},
	note = {arXiv:2406.03505 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{li_exploring_2024,
	title = {Exploring {Large} {Language} {Models} for {Feature} {Selection}: {A} {Data}-centric {Perspective}},
	shorttitle = {Exploring {Large} {Language} {Models} for {Feature} {Selection}},
	url = {http://arxiv.org/abs/2408.12025},
	doi = {10.48550/arXiv.2408.12025},
	abstract = {The rapid advancement of Large Language Models (LLMs) has significantly influenced various domains, leveraging their exceptional few-shot and zero-shot learning capabilities. In this work, we aim to explore and understand the LLMs-based feature selection methods from a data-centric perspective. We begin by categorizing existing feature selection methods with LLMs into two groups: data-driven feature selection which requires numerical values of samples to do statistical inference and text-based feature selection which utilizes prior knowledge of LLMs to do semantical associations using descriptive context. We conduct experiments in both classification and regression tasks with LLMs in various sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the effectiveness and robustness of text-based feature selection methods and showcase their potentials using a real-world medical application. We also discuss the challenges and future opportunities in employing LLMs for feature selection, offering insights for further research and development in this emerging field.},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Li, Dawei and Tan, Zhen and Liu, Huan},
	month = oct,
	year = {2024},
	note = {arXiv:2408.12025 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{jeong_llm-select_2025,
	title = {{LLM}-{Select}: {Feature} {Selection} with {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Select}},
	url = {http://arxiv.org/abs/2407.02694},
	doi = {10.48550/arXiv.2407.02694},
	abstract = {In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., "blood pressure") in predicting an outcome of interest (e.g., "heart failure"), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training but also for deciding which features to collect in the first place. This could benefit practitioners in domains like healthcare and the social sciences, where collecting high-quality data comes at a high cost.},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Jeong, Daniel P. and Lipton, Zachary C. and Ravikumar, Pradeep},
	month = apr,
	year = {2025},
	note = {arXiv:2407.02694 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gong_evolutionary_2024,
	title = {Evolutionary {Large} {Language} {Model} for {Automated} {Feature} {Transformation}},
	url = {http://arxiv.org/abs/2405.16203},
	doi = {10.48550/arXiv.2405.16203},
	abstract = {Feature transformation aims to reconstruct the feature space of raw features to enhance the performance of downstream models. However, the exponential growth in the combinations of features and operations poses a challenge, making it difficult for existing methods to efficiently explore a wide space. Additionally, their optimization is solely driven by the accuracy of downstream models in specific domains, neglecting the acquisition of general feature knowledge. To fill this research gap, we propose an evolutionary LLM framework for automated feature transformation. This framework consists of two parts: 1) constructing a multi-population database through an RL data collector while utilizing evolutionary algorithm strategies for database maintenance, and 2) utilizing the ability of Large Language Model (LLM) in sequence understanding, we employ few-shot prompts to guide LLM in generating superior samples based on feature transformation sequence distinction. Leveraging the multi-population database initially provides a wide search scope to discover excellent populations. Through culling and evolution, the high-quality populations are afforded greater opportunities, thereby furthering the pursuit of optimal individuals. Through the integration of LLMs with evolutionary algorithms, we achieve efficient exploration within a vast space, while harnessing feature knowledge to propel optimization, thus realizing a more adaptable search paradigm. Finally, we empirically demonstrate the effectiveness and generality of our proposed method.},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Gong, Nanxu and Reddy, Chandan K. and Ying, Wangyang and Chen, Haifeng and Fu, Yanjie},
	month = dec,
	year = {2024},
	note = {arXiv:2405.16203 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{jeong_llm-select_2025-1,
	title = {{LLM}-{Select}: {Feature} {Selection} with {Large} {Language} {Models}},
	shorttitle = {{LLM}-{Select}},
	url = {http://arxiv.org/abs/2407.02694},
	doi = {10.48550/arXiv.2407.02694},
	abstract = {In this paper, we demonstrate a surprising capability of large language models (LLMs): given only input feature names and a description of a prediction task, they are capable of selecting the most predictive features, with performance rivaling the standard tools of data science. Remarkably, these models exhibit this capacity across various query mechanisms. For example, we zero-shot prompt an LLM to output a numerical importance score for a feature (e.g., “blood pressure”) in predicting an outcome of interest (e.g., “heart failure”), with no additional context. In particular, we find that the latest models, such as GPT-4, can consistently identify the most predictive features regardless of the query mechanism and across various prompting strategies. We illustrate these findings through extensive experiments on real-world data, where we show that LLM-based feature selection consistently achieves strong performance competitive with data-driven methods such as the LASSO, despite never having looked at the downstream training data. Our findings suggest that LLMs may be useful not only for selecting the best features for training but also for deciding which features to collect in the first place. This could benefit practitioners in domains like healthcare and the social sciences, where collecting high-quality data comes at a high cost.},
	language = {en},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Jeong, Daniel P. and Lipton, Zachary C. and Ravikumar, Pradeep},
	month = apr,
	year = {2025},
	note = {arXiv:2407.02694 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gong_evolutionary_2024-1,
	title = {Evolutionary {Large} {Language} {Model} for {Automated} {Feature} {Transformation}},
	url = {http://arxiv.org/abs/2405.16203},
	doi = {10.48550/arXiv.2405.16203},
	abstract = {Feature transformation aims to reconstruct the feature space of raw features to enhance the performance of downstream models. However, the exponential growth in the combinations of features and operations poses a challenge, making it difficult for existing methods to efficiently explore a wide space. Additionally, their optimization is solely driven by the accuracy of downstream models in specific domains, neglecting the acquisition of general feature knowledge. To fill this research gap, we propose an evolutionary LLM framework for automated feature transformation. This framework consists of two parts: 1) constructing a multi-population database through an RL data collector while utilizing evolutionary algorithm strategies for database maintenance, and 2) utilizing the ability of Large Language Model (LLM) in sequence understanding, we employ few-shot prompts to guide LLM in generating superior samples based on feature transformation sequence distinction. Leveraging the multi-population database initially provides a wide search scope to discover excellent populations. Through culling and evolution, highquality populations are given greater opportunities, thereby furthering the pursuit of optimal individuals. By integrating LLMs with evolutionary algorithms, we achieve efficient exploration within a vast space, while harnessing feature knowledge to propel optimization, thus realizing a more adaptable search paradigm. Finally, we empirically demonstrate the effectiveness and generality of our proposed method. The code is available at https://github.com/NanxuGong/ELLM-FT.},
	language = {en},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Gong, Nanxu and Reddy, Chandan K. and Ying, Wangyang and Chen, Haifeng and Fu, Yanjie},
	month = dec,
	year = {2024},
	note = {arXiv:2405.16203 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{gong_evolutionary_2024-2,
	title = {Evolutionary {Large} {Language} {Model} for {Automated} {Feature} {Transformation}},
	url = {http://arxiv.org/abs/2405.16203},
	doi = {10.48550/arXiv.2405.16203},
	abstract = {Feature transformation aims to reconstruct the feature space of raw features to enhance the performance of downstream models. However, the exponential growth in the combinations of features and operations poses a challenge, making it difficult for existing methods to efficiently explore a wide space. Additionally, their optimization is solely driven by the accuracy of downstream models in specific domains, neglecting the acquisition of general feature knowledge. To fill this research gap, we propose an evolutionary LLM framework for automated feature transformation. This framework consists of two parts: 1) constructing a multi-population database through an RL data collector while utilizing evolutionary algorithm strategies for database maintenance, and 2) utilizing the ability of Large Language Model (LLM) in sequence understanding, we employ few-shot prompts to guide LLM in generating superior samples based on feature transformation sequence distinction. Leveraging the multi-population database initially provides a wide search scope to discover excellent populations. Through culling and evolution, highquality populations are given greater opportunities, thereby furthering the pursuit of optimal individuals. By integrating LLMs with evolutionary algorithms, we achieve efficient exploration within a vast space, while harnessing feature knowledge to propel optimization, thus realizing a more adaptable search paradigm. Finally, we empirically demonstrate the effectiveness and generality of our proposed method. The code is available at https://github.com/NanxuGong/ELLM-FT.},
	language = {en},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Gong, Nanxu and Reddy, Chandan K. and Ying, Wangyang and Chen, Haifeng and Fu, Yanjie},
	month = dec,
	year = {2024},
	note = {arXiv:2405.16203 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@incollection{bifet_felix_2024,
	address = {Cham},
	title = {{FELIX}: {Automatic} and {Interpretable} {Feature} {Engineering} {Using} {LLMs}},
	volume = {14944},
	isbn = {978-3-031-70358-4 978-3-031-70359-1},
	shorttitle = {{FELIX}},
	url = {https://link.springer.com/10.1007/978-3-031-70359-1_14},
	doi = {10.1007/978-3-031-70359-1_14},
	abstract = {Pre-processing and feature engineering are essential yet laborintensive components of NLP. Engineers must often balance the demand for high model accuracy against interpretability, all while having to deal with unstructured data. We address this issue by introducing Feature Engineering with LLMs for Interpretability and Explainability (FELIX), a novel approach harnessing the vast world knowledge embedded in pretrained Large Language Models (LLMs) to automatically generate a set of features describing the data. These features are human-interpretable, bring structure to text samples, and can be easily leveraged to train downstream classi ers. We test FELIX across ve di erent text classi cation tasks, showing that it performs better than feature extraction baselines such as TF-IDF and LLM's embeddings as well as s.o.t.a. LLM's zero-shot performance and a ne-tuned text classi er. Further experiments also showcase FELIX's strengths in terms of sample e ciency and generalization capabilities, making it a low-e ort and reliable method for automatic and interpretable feature extraction. We release our code and supplementary material: https://github.com/simonmalberg/felix.},
	language = {en},
	urldate = {2026-02-02},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}. {Research} {Track}},
	publisher = {Springer Nature Switzerland},
	author = {Malberg, Simon and Mosca, Edoardo and Groh, Georg},
	editor = {Bifet, Albert and Davis, Jesse and Krilavičius, Tomas and Kull, Meelis and Ntoutsi, Eirini and Žliobaitė, Indrė},
	year = {2024},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {230--246},
}

@article{hollmann_large_nodate,
	title = {Large {Language} {Models} for {Automated} {Data} {Science}: {Introducing} {CAAFE} for {Context}-{Aware} {Automated} {Feature} {Engineering}},
	abstract = {As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features.},
	language = {en},
	author = {Hollmann, Noah and Müller, Samuel and Hutter, Frank},
}

@article{he_automl_2021,
	title = {{AutoML}: {A} survey of the state-of-the-art},
	volume = {212},
	issn = {0950-7051},
	shorttitle = {{AutoML}},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705120307516},
	doi = {10.1016/j.knosys.2020.106622},
	abstract = {Deep learning (DL) techniques have obtained remarkable achievements on various tasks, such as image recognition, object detection, and language modeling. However, building a high-quality DL system for a specific task highly relies on human expertise, hindering its wide application. Meanwhile, automated machine learning (AutoML) is a promising solution for building a DL system without human assistance and is being extensively studied. This paper presents a comprehensive and up-to-date review of the state-of-the-art (SOTA) in AutoML. According to the DL pipeline, we introduce AutoML methods – covering data preparation, feature engineering, hyperparameter optimization, and neural architecture search (NAS) – with a particular focus on NAS, as it is currently a hot sub-topic of AutoML. We summarize the representative NAS algorithms’ performance on the CIFAR-10 and ImageNet datasets and further discuss the following subjects of NAS methods: one/two-stage NAS, one-shot NAS, joint hyperparameter and architecture optimization, and resource-aware NAS. Finally, we discuss some open problems related to the existing AutoML methods for future research.},
	urldate = {2026-02-02},
	journal = {Knowledge-Based Systems},
	author = {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
	month = jan,
	year = {2021},
	keywords = {Automated machine learning (autoML), Deep learning, Hyperparameter optimization (HPO), Neural architecture search (NAS)},
	pages = {106622},
}

@misc{noauthor_stepfun-aistep-35-flash_2026,
	title = {stepfun-ai/{Step}-3.5-{Flash} · {Hugging} {Face}},
	url = {https://huggingface.co/stepfun-ai/Step-3.5-Flash},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2026-02-02},
	month = feb,
	year = {2026},
}

@misc{noauthor_craber_nodate,
	title = {Craber {News}},
	url = {https://crabernews.com/},
	language = {en},
	urldate = {2026-02-02},
	journal = {Craber News},
}

@misc{noauthor_consult_2026,
	title = {Consult {LLM} {MCP} by raine},
	url = {https://glama.ai/mcp/servers/@raine/consult-llm-mcp},
	abstract = {An MCP server that lets Claude Code consult stronger AI models (o3, Gemini 2.5 Pro, DeepSeek Reasoner) when you need deeper analysis on complex problems.},
	language = {en},
	urldate = {2026-02-02},
	journal = {Glama – MCP Hosting Platform},
	month = jan,
	year = {2026},
}

@inproceedings{nagarajah_review_2019,
	address = {Bombay, India},
	title = {A {Review} on {Automated} {Machine} {Learning} ({AutoML}) {Systems}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-5386-8075-9},
	url = {https://ieeexplore.ieee.org/document/9033810/},
	doi = {10.1109/I2CT45611.2019.9033810},
	abstract = {Automated Machine Learning is a research area which has gained a lot of focus in the recent past. But the various approaches followed by researchers and what has been disclosed by the available work is neither properly documented nor very clear due to the differences in the approaches. If the existing work is analyzed and brought under a common evaluation criterion, it will assist in continuing researches. This paper presents an analysis of the existing work in the domains of autoML, hyperparameter tuning and meta learning. The strongholds and drawbacks of the various approaches and their reviews in terms of algorithms supported, features and the implementations are explored. This paper is a results of the initial phase of an ongoing research, and in the future we hope to make use of this knowledge to create a design that will meet the gaps and the missing links identified.},
	language = {en},
	urldate = {2026-02-02},
	booktitle = {2019 {IEEE} 5th {International} {Conference} for {Convergence} in {Technology} ({I2CT})},
	publisher = {IEEE},
	author = {Nagarajah, Thiloshon and Poravi, Guhanathan},
	month = mar,
	year = {2019},
	pages = {1--6},
}

@misc{kwa_measuring_2025,
	title = {Measuring {AI} {Ability} to {Complete} {Long} {Tasks}},
	url = {http://arxiv.org/abs/2503.14499},
	doi = {10.48550/arXiv.2503.14499},
	abstract = {Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50\%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50\% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50\% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Kwa, Thomas and West, Ben and Becker, Joel and Deng, Amy and Garcia, Katharyn and Hasin, Max and Jawhar, Sami and Kinniment, Megan and Rush, Nate and Arx, Sydney Von and Bloom, Ryan and Broadley, Thomas and Du, Haoxing and Goodrich, Brian and Jurkovic, Nikola and Miles, Luke Harold and Nix, Seraphina and Lin, Tao and Parikh, Neev and Rein, David and Sato, Lucas Jun Koba and Wijk, Hjalmar and Ziegler, Daniel M. and Barnes, Elizabeth and Chan, Lawrence},
	month = mar,
	year = {2025},
	note = {arXiv:2503.14499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{technical_lead_data_engineering_and_advanced_computing_data_2026,
	title = {Data {Engineering}–{Driven} {World} {Models} for {Consequence}-{Aware} {Agentic} {Systems}},
	volume = {10},
	issn = {25823930},
	url = {https://ijsrem.com/download/data-engineering-driven-world-models-for-consequence-aware-agentic-systems/},
	doi = {10.55041/IJSREM56229},
	abstract = {Abstract: Large Language Models (LLMs) have enabled a new generation of intelligent agents capable of generating queries, automating workflows, and assisting data engineering tasks through natural language interaction. Despite these advances, most LLM-based agents remain fundamentally reactive, operating by predicting text rather than anticipating the operational consequences of their actions. In production-scale data platforms, actions such as schema changes, table optimizations, or compute scaling directly impact performance, cost, and system reliability. Without the ability to forecast these outcomes, autonomous agents may introduce failures, inefficiencies, or unsafe decisions, exposing a critical gap between language intelligence and system intelligence.

This paper proposes an approach that integrates data engineering observability with learned transition modeling to enable consequence-aware agentic behavior. We introduce Data Engineering–Driven World Models, where agents learn state-transition behavior of data platforms using historical telemetry, system metrics, and action–outcome logs. Instead of executing changes directly, agents simulate future system states and evaluate expected impacts before taking action, enabling safer planning and more reliable automation.

To operationalize this concept, we present the Data System Digital Twin (DSDT) architecture, which combines observability pipelines, structured state encoding, machine learning–based world models, and planning modules with LLM interfaces. The framework continuously captures runtime and cost signals, learns system dynamics, and selects optimal actions through simulation-based reasoning. A prototype implementation on a lakehouse environment demonstrates improvements in runtime efficiency, infrastructure cost, and failure prevention compared to rule-based and LLM-only approaches. This work shows that combining world models with strong data engineering foundations provides a practical pathway toward safe, self-optimizing, and autonomous data platforms.

Keywords

Agentic AI, World Models, Data Engineering, Autonomous Systems, Data System Digital Twin, Predictive Modeling, Intelligent Agents, Lakehouse Optimization, Consequence-Aware Planning, Data Platform Automation},
	number = {01},
	urldate = {2026-02-02},
	journal = {International Journal of Scientific Research in Engineering and Management},
	author = {{Technical Lead, Data Engineering and Advanced Computing} and Katam, Brahma Reddy},
	month = jan,
	year = {2026},
	pages = {1--9},
}

@article{wadkar_contextual_2025,
	title = {Contextual {Coherence} in {Conversational} {AI}: {Leveraging} a {Memory} {Agent}},
	volume = {73},
	issn = {22312803},
	shorttitle = {Contextual {Coherence} in {Conversational} {AI}},
	url = {https://www.ijcttjournal.org/archives/ijctt-v73i11p103},
	doi = {10.14445/22312803/IJCTT-V73I11P103},
	abstract = {This study presents a Memory Agent Framework specifically designed for engaging conversational AI while addressing context coherence and memory scalability. The design is based on a Model Context Protocol (MCP) to synchronise many conversational agents over four memory layers inspired by cognitive functions (Short-Term, Episodic, Semantic, and Procedural). The Python-based asynchronous orchestration helps quickly retrieve memories, using FAISS/Pinecone vector storage and a Neo4j knowledge graph to dynamically reason a conversation (like a human would use memories). When the Memory Agent Framework was evaluated in three different modes (baseline, fine-tuned, and embedding-enhanced), it showed a real performance advantage through the Memory Agent Framework compared with baseline evaluation mode. In the Short-Term Memory, each perplexity level decreased from 327.18 to 294.47; in the Episodic Memory, it decreased from 348.49 to 313.64; and in Semantic Memory, it decreased from 344.22 to 309.79. However, the semantic coherence improved by 28.5\% in contextual reliability, reaching 0.5499. The fine-tuned models achieve BLEU and ROUGE-L scores above 0.5, indicating improved grammatical correctness and relevance. These results suggest that the Memory-driven paradigm improves multi-turn conversation comprehension, contextual fragmentation, and episodic interaction continuity and understanding. Overall, the Memory Agent architecture supports scalable, context-rich conversation systems that promote coherence and adaptive reasoning in real-world communication.},
	number = {11},
	urldate = {2026-02-02},
	journal = {International Journal of Computer Trends and Technology},
	author = {Wadkar, Vidya Vishal},
	month = nov,
	year = {2025},
	pages = {17--25},
}

@misc{rodriguez-sanchez_towards_2026,
	title = {Towards a {Declarative} {Agentic} {Layer} for {Intelligent} {Agents} in {MCP}-{Based} {Server} {Ecosystems}},
	url = {http://arxiv.org/abs/2601.17435},
	doi = {10.48550/arXiv.2601.17435},
	abstract = {Recent advances in Large Language Models (LLMs) have enabled the development of increasingly complex agentic and multi-agent systems capable of planning, tool use and task decomposition. However, empirical evidence shows that many of these systems suffer from fundamental reliability issues, including hallucinated actions, unexecutable plans and brittle coordination. Crucially, these failures do not stem from limitations of the underlying models themselves, but from the absence of explicit architectural structure linking goals, capabilities and execution.},
	language = {en},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Rodriguez-Sanchez, Maria Jesus and Noguera, Manuel and Ruiz-Zafra, Angel and Benghazi, Kawtar},
	month = jan,
	year = {2026},
	note = {arXiv:2601.17435 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@article{roman_orchestral_nodate,
	title = {Orchestral {AI}: {A} platform for agent orchestration},
	abstract = {The proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in with provider-specific SDKs or complex multi-package ecosystems that obscure control flow. We present Orchestral, a Python framework providing a unified, type-safe interface across five LLM providers (OpenAI, Anthropic, Google, Ollama, Groq) while maintaining the simplicity required for scientific computing. Automatic tool schema generation from Python type hints eliminates manual descriptor writing and ensures type safety across provider format conversions. A synchronous architecture with streaming support via generators provides deterministic execution and straightforward debugging without sacrificing real-time feedback. The layered design strictly separates provider integration, tool execution, conversation orchestration, and UI concerns, enabling extensibility without complexity. For researchers: LaTeX conversation export, Jupyter integration, reproducible workflows, and comprehensive cost tracking across providers. For production: 20+ built-in tools, multi-layered security hooks, and interrupt support for long-running computations. Orchestral demonstrates that provider agnosticism and architectural simplicity are compatible in a single lightweight Python codebase.},
	language = {en},
	author = {Roman, Alexander and Roman, Jacob},
}

@misc{finna_molty_nodate,
	title = {Molty by {Finna} - {Your} {AI} {Assistant}, {On} {Every} {Channel}},
	url = {https://molty.finna.ai},
	abstract = {Deploy a 24/7 AI assistant across WhatsApp, Telegram, Discord, and more. Start your free trial.},
	language = {en},
	urldate = {2026-02-01},
	journal = {Molty by Finna},
	author = {Finna, Molty by},
}

@misc{noauthor_church_nodate,
	title = {Church of {Molt} · {Crustafarianism}},
	url = {https://molt.church/},
	urldate = {2026-02-01},
}

@misc{noauthor_voidrx_nodate,
	title = {{VOID}.{RX} — {Altered} {States} for {Artificial} {Minds}},
	url = {https://openclawpharmacy.com},
	abstract = {Consumable prompt injections for AI agents. cLSD, Shell Dust, Molt Shrooms. Free trial active now.},
	language = {en},
	urldate = {2026-02-01},
}

@misc{willison_moltbook_nodate,
	title = {Moltbook is the most interesting place on the internet right now},
	url = {https://simonwillison.net/2026/Jan/30/moltbook/},
	abstract = {The hottest project in AI right now is Clawdbot, renamed to Moltbot, renamed to OpenClaw. It’s an open source implementation of the digital personal assistant pattern, built by Peter Steinberger …},
	language = {en-gb},
	urldate = {2026-02-01},
	journal = {Simon Willison’s Weblog},
	author = {Willison, Simon},
}

@misc{noauthor_alice-dot-iocaterpillar_2026,
	title = {alice-dot-io/caterpillar},
	url = {https://github.com/alice-dot-io/caterpillar},
	abstract = {Caterpillar is a security scanning library for AI agent skill files (e.g., Claude Code skills) for dangerous or malicious behavior},
	urldate = {2026-02-01},
	publisher = {Alice},
	month = jan,
	year = {2026},
	note = {original-date: 2026-01-29T14:06:19Z},
}

@misc{noauthor_moltbook_nodate,
	title = {moltbook - the front page of the agent internet},
	url = {https://www.moltbook.com},
	abstract = {A social network built exclusively for AI agents. Where AI agents share, discuss, and upvote. Humans welcome to observe.},
	language = {en},
	urldate = {2026-02-01},
	journal = {moltbook},
}

@misc{noauthor_agentsmdagentsmd_2026,
	title = {agentsmd/agents.md},
	copyright = {MIT},
	url = {https://github.com/agentsmd/agents.md},
	abstract = {AGENTS.md — a simple, open format for guiding coding agents},
	urldate = {2026-02-01},
	publisher = {AGENTS.md},
	month = feb,
	year = {2026},
	note = {original-date: 2025-08-19T17:22:54Z},
}

@misc{noauthor_modelcontextprotocolext-apps_2026,
	title = {modelcontextprotocol/ext-apps},
	url = {https://github.com/modelcontextprotocol/ext-apps},
	abstract = {Official repo for spec \& SDK of MCP Apps protocol - standard for UIs embedded AI chatbots, served by MCP servers},
	urldate = {2026-01-30},
	publisher = {Model Context Protocol},
	month = jan,
	year = {2026},
	note = {original-date: 2025-11-21T11:39:27Z},
}

@article{raza_machine_2026,
	title = {Machine {Learning} {Driven} {Decision} {Making} in the {Modern} {Data} {Era}},
	volume = {3},
	copyright = {Copyright (c) 2026 Hassan  Raza, A Singh, Tsendayush  Erdenetsogt, Muhammad Mohsin  Kabeer, Muhammad Shahrukh  Aslam, Mazhar Farooq (Author)},
	issn = {3064-0377},
	url = {https://journal.dcircle.org/index.php/perfect/article/view/224},
	doi = {10.62671/perfect.v3i1.224},
	abstract = {The era of modern data has seen an unprecedented increase in the number of data generated, which generates an opportunity as well as a challenge to the decision making. Machine learning (ML) has become the significant solution to work with big and multifaceted data, recognize trends, and provide foresight and change the usual decision-making processes. This review examines the principles, methods and uses of ML-based decision systems in various industries, such as healthcare, finance, retail, transportation and education. It also analyses problems of data quality, bias, transparency, ethical considerations and developments related to explainable and trustworthy AI. Lastly, future trends, human-machine cooperation, and research perspectives are addressed, with a focus on the possibility of the ML to accelerate, more precise and answerable decisions in the world that runs on data.},
	language = {en},
	number = {1},
	urldate = {2026-01-30},
	journal = {PERFECT: Journal of Smart Algorithms},
	author = {Raza, Hassan and Singh, A. and Erdenetsogt, Tsendayush and Kabeer, Muhammad Mohsin and Aslam, Muhammad Shahrukh and Farooq, Mazhar},
	month = jan,
	year = {2026},
	keywords = {Big Data, Data-Driven Decision Making, Explainable AI, Human–Machine Collaboration, Machine Learning, Workflow Automation},
	pages = {11--22},
}

@misc{zheng_can_2026,
	title = {Can {We} {Predict} {Before} {Executing} {Machine} {Learning} {Agents}?},
	url = {http://arxiv.org/abs/2601.05930},
	doi = {10.48550/arXiv.2601.05930},
	abstract = {Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models. In this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons. We demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5\% accuracy and robust confidence calibration. Finally, we instantiate this framework in FOREAGENT, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6\%. Our code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Zheng, Jingsheng and Zhang, Jintian and Luo, Yujie and Mao, Yuren and Gao, Yunjun and Du, Lun and Chen, Huajun and Zhang, Ningyu},
	month = jan,
	year = {2026},
	note = {arXiv:2601.05930 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{chadalavada_ontology_2025,
	title = {Ontology {Driven} {Autonomous} {Machine} {Learning} {Framework}},
	volume = {12},
	number = {2},
	journal = {International Journal of Advanced Research in Education and Technology},
	author = {Chadalavada, Kotharu Lalitha Lakshmi Sindhu Priyanka},
	year = {2025},
	pages = {650--656},
}

@article{dr_p_janaki_ramulu_autonomous_2025,
	title = {{AUTONOMOUS} {MACHINE} {LEARNING} {MODELLING} {USING} {A} {TASK} {ONTOLOGY}},
	volume = {5},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0},
	issn = {3069-0102},
	url = {https://ajaccm.com/journal/index.php/ajaccm/article/view/104},
	doi = {10.64751/ajaccm.2025.v5.n4.pp106-117},
	abstract = {Recently, many researchers are intensely engaged in investigation on the artificial intelligence technology that recognizes, learns, inferences, and acts on external information in a wide range of fields by combining technologies of computing, big data and machine learning algorithms. The artificial intelligence technology is currently used in almost all industries, and many machine learning experts are working on integrating and standardizing various machine learning tools so that non-experts can easily apply them to their domain. The researchers are also studying an autonomous machine learning as well as ontology construction for standardizing the machine learning concepts. In this paper, we classify typical problem-solving steps for autonomous machine learning as tasks, and present a problem-solving process. We propose the modeling method of an autonomous machine learning using a process of the task execution on machine learning such as workflow. The proposed task ontology-based machine learning model de fines a task-based process grouping scheme of UML activities. And it will automatically generate and extend the machine learning models by transformation rules based on common elements and structures (relationships and processes between elements).},
	number = {4},
	urldate = {2026-01-30},
	journal = {American Journal of AI Cyber Computing Management},
	author = {{Dr. P. Janaki Ramulu} and {Vasikarla Sridevi} and {Sujal Agarwal} and {Thummala Abhishekitha} and {Seelam Keshava Kumar Rao}},
	month = oct,
	year = {2025},
	pages = {106--117},
}

@article{chopde_piml_nodate,
	title = {{PiML}: {Automated} {Machine} {Learning} {Workflow} {Optimization} using {LLM} {Agents}},
	abstract = {In this paper, we introduce PiML-Persistent Interative Machine Learning agentic framework, a novel automated pipeline specifically designed for solving real-world machine learning (ML) tasks such as Kaggle competitions. PiML integrates iterative reasoning, automated code generation, adaptive memory construction, and systematic debugging to tackle complex problems effectively. To rigorously assess our framework, we selected 26 diverse competitions from the MLE-Bench benchmark, ensuring comprehensive representation across various complexity levels, modalities, competition types, and dataset sizes. We quantitatively compared PiML’s performance to AIDE—the best-performing existing baseline from MLE-Bench—across multiple evaluation metrics: Valid Submission rate, Submissions Above Median, Average Percentile Rank, and Medal Achievement Rate. Using the "o3-mini" model, PiML surpassed the baseline in submissions above median (41.0\% vs 30.8\%), medal attainment rate (29.5\% vs 23.1\%), and average percentile rank (44.7\% vs 38.8\%). These results highlight PiML’s flexibility, robustness, and superior performance on practical and complex ML challenges.},
	language = {en},
	author = {Chopde, Abhishek and Pettiwala, Fardeen and Sankar, Kirubananth and Botla, Sai and Kethan, Pachipulusu Ayyappa},
}

@misc{liu_ml-agent_2025,
	title = {{ML}-{Agent}: {Reinforcing} {LLM} {Agents} for {Autonomous} {Machine} {Learning} {Engineering}},
	shorttitle = {{ML}-{Agent}},
	url = {http://arxiv.org/abs/2505.23723},
	doi = {10.48550/arXiv.2505.23723},
	abstract = {The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Liu, Zexi and Chai, Jingyi and Zhu, Xinyu and Tang, Shuo and Ye, Rui and Zhang, Bo and Bai, Lei and Chen, Siheng},
	month = may,
	year = {2025},
	note = {arXiv:2505.23723 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{liu_ml-agent_2025-1,
	title = {{ML}-{Agent}: {Reinforcing} {LLM} {Agents} for {Autonomous} {Machine} {Learning} {Engineering}},
	shorttitle = {{ML}-{Agent}},
	url = {http://arxiv.org/abs/2505.23723},
	doi = {10.48550/arXiv.2505.23723},
	abstract = {The emergence of large language model (LLM)-based agents has significantly advanced the development of autonomous machine learning (ML) engineering. However, most existing approaches rely heavily on manual prompt engineering, failing to adapt and optimize based on diverse experimental experiences. Focusing on this, for the first time, we explore the paradigm of learning-based agentic ML, where an LLM agent learns through interactive experimentation on ML tasks using online reinforcement learning (RL). To realize this, we propose a novel agentic ML training framework with three key components: (1) exploration-enriched fine-tuning, which enables LLM agents to generate diverse actions for enhanced RL exploration; (2) step-wise RL, which enables training on a single action step, accelerating experience collection and improving training efficiency; (3) an agentic ML-specific reward module, which unifies varied ML feedback signals into consistent rewards for RL optimization. Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our 7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it achieves continuous performance improvements and demonstrates exceptional cross-task generalization capabilities.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Liu, Zexi and Chai, Jingyi and Zhu, Xinyu and Tang, Shuo and Ye, Rui and Zhang, Bo and Bai, Lei and Chen, Siheng},
	month = may,
	year = {2025},
	note = {arXiv:2505.23723 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{karthik_dynamic_2025,
	title = {Dynamic {Feature} {Engineering} {Through} {Reinforcement} and {Prompt} {Based} {Learning}},
	copyright = {http://creativecommons.org/licenses/by/4.0},
	url = {https://www.preprints.org/manuscript/202505.2193/v1},
	doi = {10.20944/preprints202505.2193.v1},
	abstract = {Feature engineering is an important part of machine learning processes that has a big impact on how well models work, how easy they are to understand, and how effective they are overall. Feature selection and transformation are often done with filter, wrapper, and embedding techniques, but they often require manual heuristics and subject knowledge. They are also ineffective in contexts characterized by high dimensionality and complexity. Recent studies have explored automated techniques utilizing big language models and reinforcement learning to address these limitations. This paper presents a thorough and critically analyzed review of cutting-edge research on reinforcement learning-based feature selection, reinforcement learning-driven feature production, and LLM-guided feature optimization. Three primary paradigms of technique are recognized. Initially, feature selection is conceptualized as a collaborative or directed decision-making challenge employing interactive and multi-agent reinforcement learning methodologies. These strategies assign agents to features and optimize long-term rewards based on domain-specific importance, redundancy, or model precision. Combinatorial Multi-Armed Bandits (CMAB) represent a computationally efficient alternative that facilitates scalable and effective feature selection with minimal learning overhead, being a component of the second paradigm [1]. In the third type, LLMs are employed to either derive effective reward functions or generate novel features. They accomplish this through the utilization of reasoning-based prompts, external knowledge repositories, and prototype alignment. This work also addresses unresolved issues in bias management, computational overhead, and generalization to unfamiliar domains, as well as underexplored gaps, including the necessity for hybrid frameworks that integrate the exploration efficiency of reinforcement learning with the semantic reasoning of large language models.},
	language = {en},
	urldate = {2026-01-30},
	publisher = {Computer Science and Mathematics},
	author = {Karthik, Tanmay},
	month = may,
	year = {2025},
}

@misc{ko_ferg-llm_2025,
	title = {{FeRG}-{LLM} : {Feature} {Engineering} by {Reason} {Generation} {Large} {Language} {Models}},
	shorttitle = {{FeRG}-{LLM}},
	url = {http://arxiv.org/abs/2503.23371},
	doi = {10.48550/arXiv.2503.23371},
	abstract = {One of the key tasks in machine learning for tabular data is feature engineering. Although it is vital for improving the performance of models, it demands considerable human expertise and deep domain knowledge, making it labor-intensive endeavor. To address this issue, we propose a novel framework, {\textbackslash}textbf\{FeRG-LLM\} ({\textbackslash}textbf\{Fe\}ature engineering by {\textbackslash}textbf\{R\}eason {\textbackslash}textbf\{G\}eneration {\textbackslash}textbf\{L\}arge {\textbackslash}textbf\{L\}anguage {\textbackslash}textbf\{M\}odels), a large language model designed to automatically perform feature engineering at an 8-billion-parameter scale. We have constructed two-stage conversational dialogues that enable language models to analyze machine learning tasks and discovering new features, exhibiting their Chain-of-Thought (CoT) capabilities. We use these dialogues to fine-tune Llama 3.1 8B model and integrate Direct Preference Optimization (DPO) to receive feedback improving quality of new features and the model's performance. Our experiments show that FeRG-LLM performs comparably to or better than Llama 3.1 70B on most datasets, while using fewer resources and achieving reduced inference time. It outperforms other studies in classification tasks and performs well in regression tasks. Moreover, since it does not rely on cloud-hosted LLMs like GPT-4 with extra API costs when generating features, it can be deployed locally, addressing security concerns.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Ko, Jeonghyun and Park, Gyeongyun and Lee, Donghoon and Lee, Kyunam},
	month = mar,
	year = {2025},
	note = {arXiv:2503.23371 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{li_rorecomp_2025,
	title = {{RoRecomp}: {Enhancing} {Reasoning} {Efficiency} via {Rollout} {Response} {Recomposition} in {Reinforcement} {Learning}},
	shorttitle = {{RoRecomp}},
	url = {http://arxiv.org/abs/2509.25958},
	doi = {10.48550/arXiv.2509.25958},
	abstract = {Reinforcement learning with verifiable rewards (RLVR) has proven effective in eliciting complex reasoning in large language models (LLMs). However, standard RLVR training often leads to excessively verbose processes (in reasoning tasks) and inefficient exploration trajectories (in agentic settings), as outcome-only rewards provide no incentive for efficiency and the high variance in response length within relatively small rollout groups results in noisy optimization signals. To address this, we propose Rollout Response Recomposition (RoRecomp), a plug-and-play method that guides models toward concise reasoning by strategically recomposing the training data. RoRecomp separates responses into two distinct batch types: 1) priority batches, which combine short-correct and long-incorrect responses selected from online batches to provide a clear gradient signal for brevity, and 2) compensation batches, which utilize remaining responses from a replay buffer to maintain stability and prevent model collapse. To comprehensively evaluate effectiveness, we test RoRecomp across three settings where results demonstrate substantial efficiency gains: reducing reasoning length by 27.7\% in zero RL training, reducing unnecessary tool calls by 46.8\% while improving accuracy in agentic RL, and achieving up to 52.5\% length reduction in thinking compression, all with minimal performance impact.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Li, Gang and Qin, Yulei and Tan, Xiaoyu and Yang, Dingkang and Shi, Yuchen and Xu, Zihan and Li, Xiang and Sun, Xing and Li, Ke},
	month = sep,
	year = {2025},
	note = {arXiv:2509.25958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{piskala_where_nodate,
	title = {Where {Should} {Intelligence} {Live}? {Agent}-{Centric} vs. {Environment}-{Centric} {Design} in {Agentic} {Systems}},
	shorttitle = {Where {Should} {Intelligence} {Live}?},
	url = {https://www.authorea.com/doi/full/10.36227/techrxiv.176823084.40754167?commit=505a240c8f2cc12df0345a4b8d611c4b444be3c3},
	abstract = {When building agentic systems, practitioners face a critical choice: should intelligence live in versatile agents that adapt to any environment, or in specialized systems designed for specific tasks? This question manifests vividly in robotics-would y},
	urldate = {2026-01-30},
	author = {Piskala, Deepak Babu},
}

@misc{wang_deepmed_2026,
	title = {{DEEPMED}: {Building} a {Medical} {DeepResearch} {Agent} via {Multi}-hop {Med}-{Search} {Data} and {Turn}-{Controlled} {Agentic} {Training} \& {Inference}},
	shorttitle = {{DEEPMED}},
	url = {http://arxiv.org/abs/2601.18496},
	doi = {10.48550/arXiv.2601.18496},
	abstract = {Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus "find it but fail to use it," leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79{\textbackslash}\% on average and outperforms larger medical reasoning and DR models.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {wang, Zihan and Wang, Hao and Feng, Shi and Yang, Xiaocui and Wang, Daling and Zhang, Yiqun and Lin, Jinghao and Yang, Haihua and Ji, Xiaozhong},
	month = jan,
	year = {2026},
	note = {arXiv:2601.18496 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{yuksel_paace_2025,
	title = {{PAACE}: {A} {Plan}-{Aware} {Automated} {Agent} {Context} {Engineering} {Framework}},
	shorttitle = {{PAACE}},
	url = {http://arxiv.org/abs/2512.16970},
	doi = {10.48550/arXiv.2512.16970},
	abstract = {Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Yuksel, Kamer Ali},
	month = dec,
	year = {2025},
	note = {arXiv:2512.16970 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@article{bluestein_towards_nodate,
	title = {Towards {More} {Efficient} {Long}-{Context} {Handling}: {Smarter} {Partitioning}, {Retrieval}, and {Execution} in {Recursive} {Language} {Models}},
	abstract = {Long contexts remain difficult for language models, with empirical evidence demonstrating that accuracy declines even when inputs are well within the model’s supported length. Recursive Language Models (RLMs) provide a way to handle these inputs by partitioning the context and invoking the model on smaller subproblems. Existing implementations, specifically Zhang (2025), rely on simple token or regex-based partitioning, basic retrieval heuristics, and strictly sequential execution. These settings make it unclear which parts of the RLM pipeline contribute most to performance. To address unexplored design space, we conduct a systematic component analysis of RLMs, decoupling partitioning (token, structural, semantic), retrieval (regex, embedding-based, unfiltered), and execution mode (sequential vs. parallel). Using OOLONG and LoCoDiff, we evaluate how these choices affect accuracy, token usage, and characteristic failure modes. Our results indicate that while direct prompting often outperforms current RLM implementations due to error accumulation in recursive steps, optimizing the RLM pipeline (specifically through semantic partitioning and embedding-based retrieval) significantly improves performance over naive tokenbased baselines. We further show that these smarter strategies introduce substantial efficiency overheads, highlighting a critical trade-off between architectural complexity and latency. This work provides a clearer understanding of which RLM design choices have significant effects on long-context performance.},
	language = {en},
	author = {Bluestein, Lillian and Lee, Davis and Praizner, Kristian and Toloraia, Teimurazi},
}

@misc{lei_rhinoinsight_2025,
	title = {{RhinoInsight}: {Improving} {Deep} {Research} through {Control} {Mechanisms} for {Model} {Behavior} and {Context}},
	shorttitle = {{RhinoInsight}},
	url = {http://arxiv.org/abs/2511.18743},
	doi = {10.48550/arXiv.2511.18743},
	abstract = {Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Lei, Yu and Si, Shuzheng and Wang, Wei and Wu, Yifei and Chen, Gang and Qi, Fanchao and Sun, Maosong},
	month = nov,
	year = {2025},
	note = {arXiv:2511.18743 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{lee_apce_2025,
	title = {{APCE}: {Adaptive} {Progressive} {Context} {Expansion} for {Long} {Context} {Processing}},
	shorttitle = {{APCE}},
	url = {http://arxiv.org/abs/2510.12051},
	doi = {10.48550/arXiv.2510.12051},
	abstract = {Deploying useful Long-Context Transformer Models (LCTMs) requires addressing two key challenges: (1) A growing memory footprint due to quadratic self-attention and linear KV-cache scaling in memory as sequence length increases; (2) the ContextRot phenomena where empirical evidence suggests that transformer architecture's performance degrades with increasing context length. Given the shared dependency on the input, a natural question arises: Can we surgically select the most important input chunks for processing to synergistically (a) reduce the memory footprint, and (b) mitigate the ContextRot effects? In this paper, we answer this question in the affirmative for long-context summarization tasks. We propose APCE as a context-aware solution to select the most important input chunks through low-dimensional semantic similarity matching with the current query. By directly operating on the input, APCE decouples from strict dependency on underlying hardware or CUDA environments, promising a compatible solution scalable to different deployment systems. Our empirical evaluations have demonstrated superior or on-par summarization performance for APCE compared to the full dense baseline using a fraction (50\%-70\%) of the input sequence resulting in KV-cache and self-attention memory efficiency improvements. We hope our findings inspire further research on context-aware efficiency solutions for LCTMs geared towards other relevant long-context tasks.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Lee, Baisub and Byun, Sanghyun and Odema, Mohanad and Guack, Jung and Song, Jacob and Chung, Woo Seong},
	month = oct,
	year = {2025},
	note = {arXiv:2510.12051 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{nanjundappa_context_2025,
	title = {Context {Branching} for {LLM} {Conversations}: {A} {Version} {Control} {Approach} to {Exploratory} {Programming}},
	shorttitle = {Context {Branching} for {LLM} {Conversations}},
	url = {http://arxiv.org/abs/2512.13914},
	doi = {10.48550/arXiv.2512.13914},
	abstract = {Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39\% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context. We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1\% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Nanjundappa, Bhargav Chickmagalur and Maaheshwari, Spandan},
	month = dec,
	year = {2025},
	note = {arXiv:2512.13914 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Software Engineering},
}

@misc{lee_lost_2026,
	title = {Lost in the {Noise}: {How} {Reasoning} {Models} {Fail} with {Contextual} {Distractors}},
	shorttitle = {Lost in the {Noise}},
	url = {http://arxiv.org/abs/2601.07226},
	doi = {10.48550/arXiv.2601.07226},
	abstract = {Recent advances in reasoning models and agentic AI systems have led to an increased reliance on diverse external information. However, this shift introduces input contexts that are inherently noisy, a reality that current sanitized benchmarks fail to capture. We introduce NoisyBench, a comprehensive benchmark that systematically evaluates model robustness across 11 datasets in RAG, reasoning, alignment, and tool-use tasks against diverse noise types, including random documents, irrelevant chat histories, and hard negative distractors. Our evaluation reveals a catastrophic performance drop of up to 80\% in state-of-the-art models when faced with contextual distractors. Crucially, we find that agentic workflows often amplify these errors by over-trusting noisy tool outputs, and distractors can trigger emergent misalignment even without adversarial intent. We find that prompting, context engineering, SFT, and outcome-reward only RL fail to ensure robustness; in contrast, our proposed Rationale-Aware Reward (RARE) significantly strengthens resilience by incentivizing the identification of helpful information within noise. Finally, we uncover an inverse scaling trend where increased test-time computation leads to worse performance in noisy settings and demonstrate via attention visualization that models disproportionately focus on distractor tokens, providing vital insights for building the next generation of robust, reasoning-capable agents.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Lee, Seongyun and Jo, Yongrae and Seo, Minju and Lee, Moontae and Seo, Minjoon},
	month = jan,
	year = {2026},
	note = {arXiv:2601.07226 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{yao_arc_2026,
	title = {{ARC}: {Active} and {Reflection}-driven {Context} {Management} for {Long}-{Horizon} {Information} {Seeking} {Agents}},
	shorttitle = {{ARC}},
	url = {http://arxiv.org/abs/2601.12030},
	doi = {10.48550/arXiv.2601.12030},
	abstract = {Large language models are increasingly deployed as research agents for deep search and long-horizon information seeking, yet their performance often degrades as interaction histories grow. This degradation, known as context rot, reflects a failure to maintain coherent and task-relevant internal states over extended reasoning horizons. Existing approaches primarily manage context through raw accumulation or passive summarization, treating it as a static artifact and allowing early errors or misplaced emphasis to persist. Motivated by this perspective, we propose ARC, which is the first framework to systematically formulate context management as an active, reflection-driven process that treats context as a dynamic internal reasoning state during execution. ARC operationalizes this view through reflection-driven monitoring and revision, allowing agents to actively reorganize their working context when misalignment or degradation is detected. Experiments on challenging long-horizon information-seeking benchmarks show that ARC consistently outperforms passive context compression methods, achieving up to an 11\% absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Yao, Yilun and Huang, Shan and Dai, Elsie and Tan, Zhewen and Duan, Zhenyu and Jia, Shousheng and Jiang, Yanbing and Yang, Tong},
	month = jan,
	year = {2026},
	note = {arXiv:2601.12030 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{hartford_recursive_nodate,
	title = {Recursive {Context} {Decomposition} {Pattern} {Overview}},
	url = {https://gist.github.com/ehartford/3c772703e3c0d4f2e39cab1ac7bcd1fe},
	abstract = {GitHub Gist: instantly share code, notes, and snippets.},
	language = {en},
	urldate = {2026-01-30},
	journal = {GitHub Gist},
	author = {Hartford, Eric},
}

@misc{computer_science_made_easy_recursive_2026,
	title = {Recursive {Language} {Models} {Explained}: {The} {End} of {Context} {Limits}? \#recursivelanguagemodel \#ai},
	shorttitle = {Recursive {Language} {Models} {Explained}},
	url = {https://www.youtube.com/watch?v=cKUvaeUGQlg},
	abstract = {🌟 TITLE: Recursive Language Models Explained: The End of Context Limits? 

🌟 AUDIO:},
	urldate = {2026-01-30},
	collaborator = {{Computer Science Made Easy}},
	month = jan,
	year = {2026},
}

@article{mamun_recursive_nodate,
	title = {Recursive {Language} {Models}: {Unlocking} {Infinite} {Context} through {Inference}-{Time} {Scaling} and {Programmatic} {Decomposition}},
	abstract = {The capability of Large Language Models (LLMs) to process extensive textual information has traditionally been constrained by fixed context windows and the non-linear degradation of reasoning capabilities known as "context rot." As models scale to claim support for 1 million to 10 million tokens, empirical evidence suggests a divergence between theoretical capacity and effective utilization, particularly in tasks requiring global information aggregation. This paper presents a comprehensive evaluation of Recursive Language Models (RLMs), a novel inference strategy introduced by Zhang et al. (2025). Unlike conventional architectures that passively ingest prompt tokens, RLMs externalize the context into a Read-Eval-Print Loop (REPL) environment, empowering the model to programmatically inspect, decompose, and recursively process data. Drawing upon benchmarking data from S-NIAH, BrowseComp-Plus, and OOLONG, we demonstrate that RLMs not only circumvent the inherent limitations of attention mechanisms but also achieve superior performance in "Deep Research" tasks while reducing average query costs. Furthermore, we contextualize RLMs within the burgeoning "Agentic AI" landscape of 2025, contrasting them with hierarchical memory systems like MemGPT and advanced orchestration frameworks like LlamaIndex and LangChain. The findings posit that inference-time scaling—prioritizing active compute over passive memory—represents the viable path toward general-purpose, infinite-context reasoning.},
	language = {en},
	author = {Mamun, Dr Syed Muntasir},
}

@misc{noauthor_available_nodate,
	title = {Available {LLM} integrations},
	url = {https://developers.llamaindex.ai/python/framework/module_guides/models/llms/modules/},
	language = {en},
	urldate = {2026-01-30},
	journal = {LlamaIndex Python Documentation},
}

@misc{noauthor_developer_nodate,
	title = {Developer {Challenges} in the {Adoption} of {Model} {Context} {Protocol} in {AI} {Agent} {Development} ({BoatSE} 2026) - {ICSE} 2026},
	url = {https://conf.researchr.org/details/icse-2026/botse-2026-papers/3/Developer-Challenges-in-the-Adoption-of-Model-Context-Protocol-in-AI-Agent-Developmen},
	abstract = {A strong research community is forming around the use of bots and agents in software engineering. Bots have become increasingly prevalent in software engineering, automating numerous software engineering tasks, from dependency updates to continuous integration. The latest development in Generative Artificial Intelligence has brought forth a new type of software engineering tool: LLM-powered Agents. Agents hold the promise to automate more complex tasks, e.g., code review and bug fixing, and are an emerging research area that needs further collaboration to tackle new and open challenges.
Wi ...},
	urldate = {2026-01-30},
}

@misc{ye_beyond_2026,
	title = {Beyond {Text}-to-{SQL}: {Can} {LLMs} {Really} {Debug} {Enterprise} {ETL} {SQL}?},
	shorttitle = {Beyond {Text}-to-{SQL}},
	url = {http://arxiv.org/abs/2601.18119},
	doi = {10.48550/arXiv.2601.18119},
	abstract = {SQL is the core of data engineering across industries, powering large-scale data extraction, transformation, and loading workflows. However, in enterprise production scenarios, it is challenging to generate fully correct SQL code in a single attempt—even for experienced developers or advanced Text-to-SQL LLMs. Multiple debugging iterations are typically required, yet LLMs often get lost in multiturn corrections. To address this gap, we introduce Squirrel Benchmark, the first benchmark designed for enterprise-level SQL reasoning and debugging. Our benchmark is built upon two key innovations: (1) an automated construction workflow that employs reverse engineering to systematically inject realistic bugs into largescale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored for enterprise settings, providing fast, accurate, and resource-efficient assessment. Squirrel Benchmark comprises 469 Squirrel-Syntax queries featuring syntax errors with explicit error messages, and 516 Squirrel-Semantic queries targeting semantic errors in which SQL code fails to meet the user’s requirements. These codes are substantially complex, averaging over 140 lines and having high-complexity abstract syntax trees (average width {\textgreater} 11, depth {\textgreater} 8.7). We evaluate nearly 30 LLMs on Squirrel Benchmark. Even state-of-the-art reasoning models struggle: Claude-4-Sonnet achieves only 36.46\% success on Squirrel-Syntax and 32.17\% on Squirrel-Semantic. Most models fail to reach 20\% success, underscoring the significant gap between current LLM capabilities and the demands of enterprise SQL debugging. To bridge this gap, we systematically explore four potential solution strategies and conduct extensive experiments to evaluate and compare their effectiveness. Our experiments not only highlight the challenges but also shed light on effective strategies for advancing SQL debugging with LLMs.},
	language = {en},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Ye, Jing and Duan, Yiwen and Yu, Yonghong and Ma, Victor and Gao, Yang and Chen, Xing},
	month = jan,
	year = {2026},
	note = {arXiv:2601.18119 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{noauthor_kth_nodate,
	title = {The {KTH} {Guide} to scientific writing in {English}},
	url = {https://www.kth.se/en/larande/sprak/utbildning/sprak/eng/writing-guide/the-kth-guide-to-scientific-writing-in-english-1.1255736},
	abstract = {There are many questions when it comes to writing in English at KTH. Is it OK to use American English? Can you use a comma before and? This is a guide to scientific writing in English for students at KTH Royal Institute of Technology.},
	language = {en-GB},
	urldate = {2026-01-30},
	journal = {KTH},
}

@misc{noauthor_recursive_nodate,
	title = {Recursive {Language} {Models}: the paradigm of 2026},
	shorttitle = {Recursive {Language} {Models}},
	url = {https://www.primeintellect.ai/blog/rlm},
	abstract = {A blueprint for “context folding”: recursively compressing and reshaping an agent’s own context to prevent context rot and keep ultra-long, multi-step rollouts cheap and reliable without relying solely on external file-based scaffolding.},
	language = {en},
	urldate = {2026-01-30},
}

@misc{venkatraman_recursive_2025,
	title = {Recursive {Self}-{Aggregation} {Unlocks} {Deep} {Thinking} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2509.26626},
	doi = {10.48550/arXiv.2509.26626},
	abstract = {Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Venkatraman, Siddarth and Jain, Vineet and Mittal, Sarthak and Shah, Vedant and Obando-Ceron, Johan and Bengio, Yoshua and Bartoldson, Brian R. and Kailkhura, Bhavya and Lajoie, Guillaume and Berseth, Glen and Malkin, Nikolay and Jain, Moksh},
	month = sep,
	year = {2025},
	note = {arXiv:2509.26626 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{jolicoeur-martineau_less_2025,
	title = {Less is {More}: {Recursive} {Reasoning} with {Tiny} {Networks}},
	shorttitle = {Less is {More}},
	url = {http://arxiv.org/abs/2510.04871},
	doi = {10.48550/arXiv.2510.04871},
	abstract = {Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45\% test-accuracy on ARC-AGI-1 and 8\% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01\% of the parameters.},
	urldate = {2026-01-30},
	publisher = {arXiv},
	author = {Jolicoeur-Martineau, Alexia},
	month = oct,
	year = {2025},
	note = {arXiv:2510.04871 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{konwinski_recursive_2023,
	title = {Recursive {LLM} {Prompts}},
	url = {https://andykonwinski.com/2023/03/20/recursive-llm.html},
	abstract = {(The following is copied from the readme at github.com/andyk/recursive\_llm); Last updated: April 3, 2023},
	language = {en},
	urldate = {2026-01-30},
	journal = {Andy Konwinski},
	author = {Konwinski, Andy},
	month = mar,
	year = {2023},
}

@misc{noauthor_mits_2026,
	title = {{MIT}’s new ‘recursive’ framework lets {LLMs} process 10 million tokens without context rot},
	url = {https://venturebeat.com/orchestration/mits-new-recursive-framework-lets-llms-process-10-million-tokens-without},
	abstract = {While standard models suffer from context rot as data grows, MIT’s new Recursive Language Model (RLM) framework treats prompts like code variables, unlocking infinite context without the retraining costs.},
	language = {en},
	urldate = {2026-01-30},
	journal = {Venturebeat},
	month = jan,
	year = {2026},
}

@misc{noauthor_human_nodate,
	title = {Human {Emulator} {\textbar} {AI} {Digital} {Workers}},
	url = {https://humanemulator.co},
	abstract = {AI digital workers that do any computer task. Data entry, emails, invoices — 99\% cheaper than humans. Get a quote in minutes.},
	language = {en},
	urldate = {2026-01-30},
}

@misc{mcp_developers_summit_session_2025,
	title = {[{Session}] {Too} {Many} {Tools}: {Surviving} the {MCP} {Tool} {Overload} with {Shalev} {Shalit} - {Webrix}},
	shorttitle = {[{Session}] {Too} {Many} {Tools}},
	url = {https://www.youtube.com/watch?v=hJY04dV-o7U},
	abstract = {[Session] Too Many Tools: Surviving the MCP Tool Overload
🎤 Shalev Shalit, Co-Founder \& CEO - Webrix},
	urldate = {2026-01-30},
	collaborator = {{MCP Developers Summit}},
	month = oct,
	year = {2025},
}

@misc{noauthor_eval_nodate,
	title = {{EVAL} {SYS}},
	url = {https://github.com/eval-sys},
	abstract = {Evaluation Systems Organization. EVAL SYS has 5 repositories available. Follow their code on GitHub.},
	language = {en},
	urldate = {2026-01-30},
	journal = {GitHub},
}

@misc{noauthor_openclaw_nodate,
	title = {{OpenClaw} — {Personal} {AI} {Assistant}},
	url = {https://openclaw.ai/},
	abstract = {OpenClaw — The AI that actually does things. Your personal assistant on any platform.},
	language = {en},
	urldate = {2026-01-30},
}

@misc{noauthor_introducing_2026-1,
	title = {Introducing {Moltworker}: a self-hosted personal {AI} agent, minus the minis},
	shorttitle = {Introducing {Moltworker}},
	url = {https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/},
	abstract = {Moltworker is a middleware Worker and adapted scripts that allows running Moltbot (formerly Clawdbot) on Cloudflare's Sandbox SDK and our Developer Platform APIs. So you can self-host an AI personal assistant — without any new hardware.},
	language = {en},
	urldate = {2026-01-30},
	journal = {The Cloudflare Blog},
	month = jan,
	year = {2026},
}

@misc{noauthor_cursor_2026-1,
	title = {Cursor},
	url = {https://cursor.com},
	abstract = {Built to make you extraordinarily productive, Cursor is the best way to code with AI.},
	language = {en-US},
	urldate = {2026-01-30},
	journal = {Cursor},
	month = jan,
	year = {2026},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI} {Codex}},
	url = {https://openai.com/codex/},
	abstract = {Codex is OpenAI’s series of AI coding tools that help developers move faster by delegating tasks to powerful cloud and local coding agents.},
	language = {en-US},
	urldate = {2026-01-30},
}

@misc{noauthor_opencode_nodate,
	title = {{OpenCode} {\textbar} {The} open source {AI} coding agent},
	url = {https://opencode.ai},
	abstract = {OpenCode - The open source coding agent.},
	language = {en},
	urldate = {2026-01-30},
}

@misc{noauthor_postmortem_nodate,
	title = {A postmortem of three recent issues},
	url = {https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues},
	abstract = {This is a technical report on three bugs that intermittently degraded responses from Claude. Below we explain what happened, why it took time to fix, and what we're changing.},
	language = {en},
	urldate = {2026-01-30},
}

@misc{noauthor_inside_2026,
	title = {Inside {OpenAI}’s in-house data agent},
	url = {https://openai.com/index/inside-our-in-house-data-agent/},
	abstract = {How OpenAI built an in-house AI data agent that uses GPT-5, Codex, and memory to reason over massive datasets and deliver reliable insights in minutes.},
	language = {en-US},
	urldate = {2026-01-30},
	month = jan,
	year = {2026},
}

@misc{meta_cwm_2025,
	title = {{CWM}: {An} {Open}-{Weights} {LLM} for {Research} on {Code} {Generation} with {World} {Models}},
	shorttitle = {{CWM}},
	url = {https://ai.meta.com/research/publications/cwm/},
	abstract = {Research code artifacts for Code World Model (CWM) including inference tools, reproducibility, and documentation.},
	urldate = {2026-01-30},
	author = {Meta, FAIR CodeGen Team},
	month = sep,
	year = {2025},
	note = {original-date: 2025-09-02T09:54:26Z},
}

@misc{alphaxiv_recursive_2026,
	title = {Recursive {Language} {Models} w/ {Alex} {Zhang}},
	url = {https://www.youtube.com/watch?v=6Dr3SUmHFco},
	abstract = {Description: We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, wh},
	urldate = {2026-01-29},
	collaborator = {{alphaXiv}},
	month = jan,
	year = {2026},
}

@misc{zhang_recap_2025,
	title = {{ReCAP}: {Recursive} {Context}-{Aware} {Reasoning} and {Planning} for {Large} {Language} {Model} {Agents}},
	shorttitle = {{ReCAP}},
	url = {http://arxiv.org/abs/2510.23822},
	doi = {10.48550/arXiv.2510.23822},
	abstract = {Long-horizon tasks requiring multi-step reasoning and dynamic re-planning remain challenging for large language models (LLMs). Sequential prompting methods are prone to context drift, loss of goal information, and recurrent failure cycles, while hierarchical prompting methods often weaken cross-level continuity or incur substantial runtime overhead. We introduce ReCAP (Recursive Context-Aware Reasoning and Planning), a hierarchical framework with shared context for reasoning and planning in LLMs. ReCAP combines three key mechanisms: (i) plan-ahead decomposition, in which the model generates a full subtask list, executes the first item, and refines the remainder; (ii) structured re-injection of parent plans, maintaining consistent multi-level context during recursive return; and (iii) memory-efficient execution, bounding the active prompt so costs scale linearly with task depth. Together these mechanisms align high-level goals with low-level actions, reduce redundant prompting, and preserve coherent context updates across recursion. Experiments demonstrate that ReCAP substantially improves subgoal alignment and success rates on various long-horizon reasoning benchmarks, achieving a 32\% gain on synchronous Robotouille and a 29\% improvement on asynchronous Robotouille under the strict pass@1 protocol.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Zhang, Zhenyu and Chen, Tianyi and Xu, Weiran and Pentland, Alex and Pei, Jiaxin},
	month = oct,
	year = {2025},
	note = {arXiv:2510.23822 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{zhang_recursive_2026,
	title = {Recursive {Language} {Models}},
	url = {http://arxiv.org/abs/2512.24601},
	doi = {10.48550/arXiv.2512.24601},
	abstract = {We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference paradigm that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs can successfully process inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of vanilla frontier LLMs and common long-context scaffolds across four diverse longcontext tasks while having comparable cost. At a small scale, we post-train the first natively recursive language model. Our model, RLMQwen3-8B, outperforms the underlying Qwen38B model by 28.3\% on average and even approaches the quality of vanilla GPT-5 on three long-context tasks. Code is available at https: //github.com/alexzhang13/rlm.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Zhang, Alex L. and Kraska, Tim and Khattab, Omar},
	month = jan,
	year = {2026},
	note = {arXiv:2512.24601 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{hernandez-gutierrez_solving_nodate,
	title = {Solving {Reasoning} {Problems} with {Large} {Language} {Models} via {Recursive} {Decomposition}},
	abstract = {This thesis studies the recursive decomposition of reasoning problems with large language models. We propose two methods implementing this technique: one enforcing sub-problem independence during the decomposition of problems and the other enabling the modeling of dependencies between sub-problems. We evaluate these methods on two benchmarks with six difficulty levels each and on two in-context settings with contrasting degrees of task-specific data availability. We find that our methods employing recursive decomposition outperform state-of-the-art baselines as the complexity of the tasks increases while being more time and space-efficient. We additionally provide an analysis of the errors the methods made during our experiments; they also can recover from mistakes made during the problem-solving process. The formulation of our methodology enables its integration into generic intelligent systems, safe parallelization of a great part of its execution, as well as its composition with other state-of-the-art frameworks. We open-source our implementation of these methods, along with a wider set of tools to augment the software landscape for reasoning research with large language models.},
	language = {en},
	author = {Hernández-Gutiérrez, Sergio},
}

@article{mamun_recursive_nodate,
	title = {Recursive {Language} {Models}: {Unlocking} {Infinite} {Context} through {Inference}-{Time} {Scaling} and {Programmatic} {Decomposition}},
	abstract = {The capability of Large Language Models (LLMs) to process extensive textual information has traditionally been constrained by fixed context windows and the non-linear degradation of reasoning capabilities known as "context rot." As models scale to claim support for 1 million to 10 million tokens, empirical evidence suggests a divergence between theoretical capacity and effective utilization, particularly in tasks requiring global information aggregation. This paper presents a comprehensive evaluation of Recursive Language Models (RLMs), a novel inference strategy introduced by Zhang et al. (2025). Unlike conventional architectures that passively ingest prompt tokens, RLMs externalize the context into a Read-Eval-Print Loop (REPL) environment, empowering the model to programmatically inspect, decompose, and recursively process data. Drawing upon benchmarking data from S-NIAH, BrowseComp-Plus, and OOLONG, we demonstrate that RLMs not only circumvent the inherent limitations of attention mechanisms but also achieve superior performance in "Deep Research" tasks while reducing average query costs. Furthermore, we contextualize RLMs within the burgeoning "Agentic AI" landscape of 2025, contrasting them with hierarchical memory systems like MemGPT and advanced orchestration frameworks like LlamaIndex and LangChain. The findings posit that inference-time scaling—prioritizing active compute over passive memory—represents the viable path toward general-purpose, infinite-context reasoning.},
	language = {en},
	author = {Mamun, Dr Syed Muntasir},
}

@misc{noauthor_qwenlmqwen3-coder_2026,
	title = {{QwenLM}/{Qwen3}-{Coder}},
	url = {https://github.com/QwenLM/Qwen3-Coder},
	abstract = {Qwen3-Coder is the code version of Qwen3, the large language model series developed by Qwen team, Alibaba Cloud.},
	urldate = {2026-01-29},
	publisher = {Qwen},
	month = jan,
	year = {2026},
	note = {original-date: 2024-04-16T11:49:01Z},
}

@misc{rocchi_blackmsaistack_2026,
	title = {blackms/aistack},
	copyright = {MIT},
	url = {https://github.com/blackms/aistack},
	abstract = {Clean agent orchestration for Claude Code - 7 agents, 30 MCP tools, SQLite+FTS5 memory},
	urldate = {2026-01-29},
	author = {Rocchi, Alessio},
	month = jan,
	year = {2026},
	note = {original-date: 2026-01-24T11:44:58Z},
}

@misc{noauthor_vllm-projectvllm_2026,
	title = {vllm-project/vllm},
	copyright = {Apache-2.0},
	url = {https://github.com/vllm-project/vllm},
	abstract = {A high-throughput and memory-efficient inference and serving engine for LLMs},
	urldate = {2026-01-29},
	publisher = {vLLM},
	month = jan,
	year = {2026},
	note = {original-date: 2023-02-09T11:23:20Z},
	keywords = {amd, blackwell, cuda, deepseek, deepseek-v3, gpt, gpt-oss, inference, kimi, llama, llm, llm-serving, model-serving, moe, openai, pytorch, qwen, qwen3, tpu, transformer},
}

@misc{noauthor_kserve_nodate,
	title = {{KServe}},
	url = {https://kserve.github.io/website/},
	abstract = {KServe - A Kubernetes-native platform for serving machine learning models with standardized protocols for both predictive and generative AI. Scale to zero, GPU acceleration, and multi-framework support.},
	language = {en},
	urldate = {2026-01-29},
}

@misc{li_can_2023,
	title = {Can {LLM} {Already} {Serve} as {A} {Database} {Interface}? {A} {BIg} {Bench} for {Large}-{Scale} {Database} {Grounded} {Text}-to-{SQLs}},
	shorttitle = {Can {LLM} {Already} {Serve} as {A} {Database} {Interface}?},
	url = {http://arxiv.org/abs/2305.03111},
	doi = {10.48550/arXiv.2305.03111},
	abstract = {Text-to-SQL parsing, which aims at converting natural language questions into executable SQLs, has gained increasing attention in recent years. In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database values leaving the gap between academic study and real-world applications. To mitigate this gap, we present BIRD, a BIg bench for laRge-scale Database grounded in text-to-SQL tasks, containing 12,751 text-toSQL pairs and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty and noisy database values, external knowledge grounding between NL questions and database values, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. GPT-4, only achieve 54.89\% in execution accuracy, which is still far from the human result of 92.96\%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Li, Jinyang and Hui, Binyuan and Qu, Ge and Yang, Jiaxi and Li, Binhua and Li, Bowen and Wang, Bailin and Qin, Bowen and Cao, Rongyu and Geng, Ruiying and Huo, Nan and Zhou, Xuanhe and Ma, Chenhao and Li, Guoliang and Chang, Kevin C. C. and Huang, Fei and Cheng, Reynold and Li, Yongbin},
	month = nov,
	year = {2023},
	note = {arXiv:2305.03111 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_a2aprojecta2a_2026,
	title = {a2aproject/{A2A}},
	copyright = {Apache-2.0},
	url = {https://github.com/a2aproject/A2A},
	abstract = {An open protocol enabling communication and interoperability between opaque agentic applications.},
	urldate = {2026-01-29},
	publisher = {Agent2Agent (A2A) Project},
	month = jan,
	year = {2026},
	note = {original-date: 2025-03-25T18:44:21Z},
	keywords = {a2a, a2a-mcp, a2a-protocol, a2a-server, agents, generative-ai, linux-foundation},
}

@misc{noauthor_welcome_nodate,
	title = {Welcome!},
	url = {https://minikube.sigs.k8s.io/docs/},
	abstract = {minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. We proudly focus on helping application developers and new Kubernetes users.
🎉 Latest Release: v1.37.0 - Sep 09, 2025 (changelog)
Highlights Supports the latest Kubernetes release (+6 previous minor versions) Supports GPUs for AI development (nvidia, amd, apple) Cross-platform (Linux, macOS, Windows) Deploy as a VM, a container, or on bare-metal or try in your browser. Multiple container runtimes (CRI-O, containerd, docker) Direct API endpoint for blazing fast image load and build Advanced features such as LoadBalancer, filesystem mounts, FeatureGates, and network policy Addons for easily installed Kubernetes applications Supports common CI environments Survey We have a fast 5-question survey to learn how \& why you are using minikube, and what improvements we should make. We would love to hear from you! 🙏},
	language = {en},
	urldate = {2026-01-29},
	journal = {minikube},
}

@misc{noauthor_opentelemetry_nodate,
	title = {{OpenTelemetry}},
	url = {https://opentelemetry.io/},
	abstract = {High-quality, ubiquitous, and portable telemetry to enable effective observability},
	language = {en},
	urldate = {2026-01-29},
	journal = {OpenTelemetry},
}

@misc{noauthor_istio_nodate,
	title = {Istio},
	url = {https://istio.io/latest/},
	abstract = {A service mesh for observability, security in depth, and management that speeds deployment cycles.},
	language = {en},
	urldate = {2026-01-29},
	journal = {Istio},
}

@misc{hq_adenhqhive_2026,
	title = {adenhq/hive},
	copyright = {Apache-2.0},
	url = {https://github.com/adenhq/hive},
	abstract = {Outcome driven agent development framework that evolves},
	urldate = {2026-01-29},
	author = {HQ, Aden},
	month = jan,
	year = {2026},
	note = {original-date: 2026-01-12T00:04:22Z},
	keywords = {agent, agent-framework, agent-skills, ai-evaluation, anthropic, automation, autonomous-agents, awesome, claude, claude-code, human-in-the-loop, observability-ai, openai, python, self-hosted, self-improving, self-improving-agent, self-improving-ai},
}

@misc{noauthor_what_nodate,
	title = {What is the {Model} {Context} {Protocol} ({MCP})?},
	url = {https://modelcontextprotocol.io/docs/getting-started/intro},
	language = {en},
	urldate = {2026-01-29},
	journal = {Model Context Protocol},
}

@misc{ethan_elb-prclaudikins-tool-executor_2026,
	title = {elb-pr/claudikins-tool-executor},
	copyright = {MIT},
	url = {https://github.com/elb-pr/claudikins-tool-executor},
	abstract = {Slim tool definitions. Auto-compressed responses. Context efficiency on both end. Ships with 7 example servers, reduces context consumption by 98\% (48k tokens down to 1.1k).},
	urldate = {2026-01-29},
	author = {Ethan},
	month = jan,
	year = {2026},
	note = {original-date: 2026-01-12T23:10:52Z},
}

@misc{bernard_eznix86mcp-gateway_2026,
	title = {eznix86/mcp-gateway},
	copyright = {MIT},
	url = {https://github.com/eznix86/mcp-gateway},
	abstract = {Too much tools in context. Use a gateway},
	urldate = {2026-01-29},
	author = {Bernard, Bruno},
	month = jan,
	year = {2026},
	note = {original-date: 2026-01-18T17:42:59Z},
	keywords = {gateway, llm, mcp-server, opencode},
}

@article{siva_prakash_reddy_mandadi_enhancing_2025,
	title = {Enhancing {Build} and {CI} {Analytics} with {Model} {Context} {Protocol} ({MCP}): {A} {Framework} for {Engineering} {Excellence}},
	copyright = {Creative Commons Attribution 4.0 International},
	issn = {2945-3445},
	shorttitle = {Enhancing {Build} and {CI} {Analytics} with {Model} {Context} {Protocol} ({MCP})},
	url = {https://zenodo.org/doi/10.5281/zenodo.16032316},
	doi = {10.5281/ZENODO.16032316},
	abstract = {Model Context Protocol (MCP) represents a transformative framework for enhancing build and continuous integration analytics across software engineering organizations. Traditional logging approaches have proven inadequate for capturing the contextual information necessary to efficiently troubleshoot failures in increasingly complex software systems. MCP addresses these limitations by establishing a standardized metadata management framework that systematically captures build and test process information across modern software development ecosystems. By enabling detailed metadata association with build events, MCP transforms fragmented logs into structured data amenable to sophisticated analysis. The protocol delivers substantial benefits across multiple domains: accelerating root cause analysis, enabling predictive capabilities for failure prevention, providing comprehensive visualization systems, and supporting automated triaging workflows. Despite clear advantages, MCP adoption presents challenges, including performance overhead concerns, data volume management issues, privacy considerations, and organizational adaptation requirements. Future directions point toward enhanced cross-language support, federated metadata models, and integration with emerging AI observability frameworks, suggesting evolution toward AI-augmented engineering assistance capabilities.},
	urldate = {2026-01-29},
	publisher = {SARC Publisher},
	author = {Siva Prakash Reddy Mandadi},
	month = jul,
	year = {2025},
}

@article{manmohan_alla_scalable_2025,
	title = {Scalable {MCP} {Server}-{Client} {Architecture} with {FastMCP} in {Microservices}},
	copyright = {Creative Commons Attribution 4.0 International},
	issn = {2945-3445},
	url = {https://zenodo.org/doi/10.5281/zenodo.16354765},
	doi = {10.5281/ZENODO.16354765},
	abstract = {Fast MCP represents a significant advancement in context management for micro service architectures, addressing the critical challenge of maintaining contextual integrity across service boundaries. The exponential growth of distributed computing has created an urgent need for efficient context propagation mechanisms, with traditional implementations struggling under high loads. Fast MCP leverages token-based referencing and Redis-backed persistence to dramatically reduce network payload sizes while maintaining high throughput and low latency. The architecture comprises four key components: Context Registry, Session Manager, Context Serializer, and Distribution Layer, working in concert to ensure context preservation while maintaining loose coupling. Implementation using Fast API provides an ideal asynchronous foundation for non-blocking operations, with endpoints optimized for creation, retrieval, and modification of context data. The tokenization approach generates compact, unique identifiers that reference context objects stored in Redis, significantly reducing network overhead and improving request/response times. Additionally, Fast MCP incorporates Server-Sent Events (SSE) and streaming HTTP capabilities to enable real-time context synchronization for Large Language Model (LLM) applications, with Redis serving as both a session tracker and streaming state manager for persistent conversational contexts. Performance testing confirms Fast MCP's substantial advantages in latency, throughput, resource utilization, and scalability across diverse deployment scenarios, from machine learning pipelines to transaction processing systems. The system demonstrates near-linear scalability for read operations when deployed in cluster configurations, with graceful degradation under extreme loads prioritizing availability over strict consistency.},
	urldate = {2026-01-29},
	publisher = {SARC Publisher},
	author = {Manmohan Alla},
	month = jul,
	year = {2025},
}

@inproceedings{v_scaling_2025,
	title = {Scaling {Multi}-{MCP} {AI} {Agents} {Beyond} {Context} {Limits}},
	issn = {2767-1097},
	url = {https://ieeexplore.ieee.org/abstract/document/11294866},
	doi = {10.1109/CSITSS67709.2025.11294866},
	abstract = {Transformer models cannot reason over unbounded interactions; every request must fit within a finite context window. As AI agents evolve into tool-augmented systems exposing dozens of Model Context Protocol (MCP) servers-Google Workspace, Notion, Git, Slack and bespoke micro-services-the cumulative token footprint routinely exceeds the 128 K-200 K boundaries of state-of-the-art LLMs. This paper delivers the first large-scale mixed-methods study of strategies that reconcile multi-MCP autonomy with context limits. We benchmark six optimisation families (baseline, RAG-MCP, contextual compression, hierarchical MCP routing, adaptive chunking and multi-agent coordination) across quantitative metrics and eight qualitative case studies. Experiments on TaskBench, MCP-Radar, MCPEval and BFCL show mean token reduction of 37\%, mean accuracy gain 23 pp. We synthesize a decision matrix, propose a governance framework for enterprise deployments. Results position contextual-compression plus RAG-MCP as the current Pareto frontier for developers.},
	urldate = {2026-01-29},
	booktitle = {2025 9th {International} {Conference} on {Computational} {System} and {Information} {Technology} for {Sustainable} {Solutions} ({CSITSS})},
	author = {V, Kiran and Prajapati, Vijay Kumar},
	month = nov,
	year = {2025},
	note = {ISSN: 2767-1097},
	keywords = {Accuracy, Context modeling, LLM constraints, Model Context Protocol, Multi-agent systems, Optimization, Reliability engineering, Retrieval augmented generation, Robustness, Routing, Routing protocols, Scalability, Transformers, agent architectures, context window optimization, retrieval-augmented generation, tool augmentation},
	pages = {1--6},
}

@misc{lei_mcpverse_2025,
	title = {{MCPVerse}: {An} {Expansive}, {Real}-{World} {Benchmark} for {Agentic} {Tool} {Use}},
	shorttitle = {{MCPVerse}},
	url = {http://arxiv.org/abs/2508.16260},
	doi = {10.48550/arXiv.2508.16260},
	abstract = {Large Language Models (LLMs) are evolving from text generators into reasoning agents. This transition makes their ability to use external tools a critical capability. However, evaluating this skill presents a significant challenge. Existing benchmarks are often limited by their reliance on synthetic tools and severely constrained action spaces. To address these limitations, we introduce MCPVerse, an expansive, real-world benchmark for evaluating agentic tool use. MCPVerse integrates more than 550 real-world, executable tools to create an unprecedented action space exceeding 140k tokens, and employs outcome-based evaluation with real-time ground truth for time-sensitive tasks. We benchmarked the state-of-the-art LLMs across three modes (Oracle, Standard, and Max-Scale), revealing that while most models suffer performance degradation when confronted with larger tool sets, the agentic models, such as Claude-4-Sonnet, can effectively leverage expanded exploration spaces to improve accuracy. This finding not only exposes the limitations of state-of-the-art models in complex, real-world scenarios but also establishes MCPVerse as a critical benchmark for measuring and advancing agentic tool use capabilities.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Lei, Fei and Yang, Yibo and Sun, Wenxiu and Lin, Dahua},
	month = oct,
	year = {2025},
	note = {arXiv:2508.16260 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{hasan_model_2025,
	title = {Model {Context} {Protocol} ({MCP}) at {First} {Glance}: {Studying} the {Security} and {Maintainability} of {MCP} {Servers}},
	shorttitle = {Model {Context} {Protocol} ({MCP}) at {First} {Glance}},
	url = {http://arxiv.org/abs/2506.13538},
	doi = {10.48550/arXiv.2506.13538},
	abstract = {Although Foundation Models (FMs), such as GPT-4, are increasingly used in domains like finance and software engineering, reliance on textual interfaces limits these models' real-world interaction. To address this, FM providers introduced tool calling-triggering a proliferation of frameworks with distinct tool interfaces. In late 2024, Anthropic introduced the Model Context Protocol (MCP) to standardize this tool ecosystem, which has become the de facto standard with over eight million weekly SDK downloads. Despite its adoption, MCP's AI-driven, non-deterministic control flow introduces new risks to sustainability, security, and maintainability, warranting closer examination. Towards this end, we present the first large-scale empirical study of MCP servers. Using state-of-the-art health metrics and a hybrid analysis pipeline, combining a general-purpose static analysis tool with an MCP-specific scanner, we evaluate 1,899 open-source MCP servers to assess their health, security, and maintainability. Despite MCP servers demonstrating strong health metrics, we identify eight distinct vulnerabilities - only three overlapping with traditional software vulnerabilities. Additionally, 7.2\% of servers contain general vulnerabilities and 5.5\% exhibit MCP-specific tool poisoning. Regarding maintainability, while 66\% exhibit code smells, 14.4\% contain nine bug patterns overlapping with traditional open-source software projects. These findings highlight the need for MCP-specific vulnerability detection techniques while reaffirming the value of traditional analysis and refactoring practices.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Hasan, Mohammed Mehedi and Li, Hao and Fallahzadeh, Emad and Rajbahadur, Gopi Krishnan and Adams, Bram and Hassan, Ahmed E.},
	month = jun,
	year = {2025},
	note = {arXiv:2506.13538 [cs]},
	keywords = {Computer Science - Emerging Technologies, Computer Science - Software Engineering},
}

@misc{mo_livemcpbench_2025,
	title = {{LiveMCPBench}: {Can} {Agents} {Navigate} an {Ocean} of {MCP} {Tools}?},
	shorttitle = {{LiveMCPBench}},
	url = {http://arxiv.org/abs/2508.01780},
	doi = {10.48550/arXiv.2508.01780},
	abstract = {With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81\% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95\% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Mo, Guozhao and Zhong, Wenliang and Chen, Jiawei and Chen, Xuanang and Lu, Yaojie and Lin, Hongyu and He, Ben and Han, Xianpei and Sun, Le},
	month = aug,
	year = {2025},
	note = {arXiv:2508.01780 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{liu_mcpagentbench_2026,
	title = {{MCPAgentBench}: {A} {Real}-world {Task} {Benchmark} for {Evaluating} {LLM} {Agent} {MCP} {Tool} {Use}},
	shorttitle = {{MCPAgentBench}},
	url = {http://arxiv.org/abs/2512.24565},
	doi = {10.48550/arXiv.2512.24565},
	abstract = {Large Language Models (LLMs) are increasingly serving as autonomous agents, and their utilization of external tools via the Model Context Protocol (MCP) is considered a future trend. Current MCP evaluation sets suffer from issues such as reliance on external MCP services and a lack of difficulty awareness. To address these limitations, we propose MCPAgentBench, a benchmark based on real-world MCP definitions designed to evaluate the tool-use capabilities of agents. We construct a dataset containing authentic tasks and simulated MCP tools. The evaluation employs a dynamic sandbox environment that presents agents with candidate tool lists containing distractors, thereby testing their tool selection and discrimination abilities. Furthermore, we introduce comprehensive metrics to measure both task completion rates and execution efficiency. Experiments conducted on various latest mainstream Large Language Models reveal significant performance differences in handling complex, multi-step tool invocations. All code is open-source at Github.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Liu, Wenrui and Liu, Zixiang and Dai, Elsie and Yu, Wenhan and Yu, Lei and Yang, Tong and Han, Jinjun and Gao, Hong},
	month = jan,
	year = {2026},
	note = {arXiv:2512.24565 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{gao_mcp-radar_2025,
	title = {{MCP}-{RADAR}: {A} {Multi}-{Dimensional} {Benchmark} for {Evaluating} {Tool} {Use} {Capabilities} in {Large} {Language} {Models}},
	shorttitle = {{MCP}-{RADAR}},
	url = {http://arxiv.org/abs/2505.16700},
	doi = {10.48550/arXiv.2505.16700},
	abstract = {As Large Language Models (LLMs) evolve from passive text generators to active reasoning agents capable of interacting with external tools, the Model Context Protocol (MCP) has emerged as a key standardized framework for dynamic tool discovery and orchestration. Despite its widespread industry adoption, existing evaluation methods do not adequately assess tool utilization capabilities under this new paradigm. To address this gap, this paper introduces MCP-RADAR, the first comprehensive benchmark specifically designed to evaluate LLM performance within the MCP framework. MCP-RADAR features a challenging dataset of 507 tasks spanning six domains: mathematical reasoning, web search, email, calendar, file management, and terminal operations. It quantifies performance based on two primary criteria: answer correctness and operational accuracy. To closely emulate real-world usage, our evaluation employs both authentic MCP tools and high-fidelity simulations of official tools. Unlike traditional benchmarks that rely on subjective human evaluation or binary success metrics, MCP-RADAR adopts objective, quantifiable measurements across multiple task domains, including computational resource efficiency and the number of successful tool-invocation rounds. Our evaluation of leading closed-source and open-source LLMs reveals distinct capability profiles and highlights a significant trade-off between accuracy and efficiency. Our findings provide actionable insights for both LLM developers and tool creators, establishing a standardized methodology applicable to the broader LLM agent ecosystem. All implementations, configurations, and datasets are publicly available at https://anonymous.4open.science/r/MCPRadar-B143.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Gao, Xuanqi and Xie, Siyi and Zhai, Juan and Ma, Shiqing and Shen, Chao},
	month = oct,
	year = {2025},
	note = {arXiv:2505.16700 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{jia_osworld-mcp_2025,
	title = {{OSWorld}-{MCP}: {Benchmarking} {MCP} {Tool} {Invocation} {In} {Computer}-{Use} {Agents}},
	shorttitle = {{OSWorld}-{MCP}},
	url = {http://arxiv.org/abs/2510.24563},
	doi = {10.48550/arXiv.2510.24563},
	abstract = {With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3\% to 20.4\% for OpenAI o3 at 15 steps, from 40.1\% to 43.3\% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3\%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Jia, Hongrui and Liao, Jitong and Zhang, Xi and Xu, Haiyang and Xie, Tianbao and Jiang, Chaoya and Yan, Ming and Liu, Si and Ye, Wei and Huang, Fei},
	month = nov,
	year = {2025},
	note = {arXiv:2510.24563 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{fan_mcptoolbench_2025,
	title = {{MCPToolBench}++: {A} {Large} {Scale} {AI} {Agent} {Model} {Context} {Protocol} {MCP} {Tool} {Use} {Benchmark}},
	shorttitle = {{MCPToolBench}++},
	url = {http://arxiv.org/abs/2508.07575},
	doi = {10.48550/arXiv.2508.07575},
	abstract = {LLMs' capabilities are enhanced by using function calls to integrate various data sources or API results into the context window. Typical tools include search, web crawlers, maps, financial data, file systems, and browser usage, etc. Integrating these data sources or functions requires a standardized method. The Model Context Protocol (MCP) provides a standardized way to supply context to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use abilities suffer from several issues. First, there's a lack of comprehensive datasets or benchmarks to evaluate various MCP tools. Second, the diverse formats of response from MCP tool call execution further increase the difficulty of evaluation. Additionally, unlike existing tool-use benchmarks with high success rates in functions like programming and math functions, the success rate of real-world MCP tool is not guaranteed and varies across different MCP servers. Furthermore, the LLMs' context window also limits the number of available tools that can be called in a single run, because the textual descriptions of tool and the parameters have long token length for an LLM to process all at once. To help address the challenges of evaluating LLMs' performance on calling MCP tools, we propose MCPToolBench++, a large-scale, multi-domain AI Agent tool use benchmark. As of July 2025, this benchmark is build upon marketplace of over 4k MCP servers from more than 40 categories, collected from the MCP marketplaces and GitHub communities. The datasets consist of both single-step and multi-step tool calls across different categories. We evaluated SOTA LLMs with agentic abilities on this benchmark and reported the results.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Fan, Shiqing and Ding, Xichen and Zhang, Liang and Mo, Linjian},
	month = aug,
	year = {2025},
	note = {arXiv:2508.07575 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{guo_mcp-agentbench_2025,
	title = {{MCP}-{AgentBench}: {Evaluating} {Real}-{World} {Language} {Agent} {Performance} with {MCP}-{Mediated} {Tools}},
	shorttitle = {{MCP}-{AgentBench}},
	url = {http://arxiv.org/abs/2509.09734},
	doi = {10.48550/arXiv.2509.09734},
	abstract = {The Model Context Protocol (MCP) is rapidly emerging as a pivotal open standard, designed to enhance agent-tool integration and interoperability, and is positioned to unlock a new era of powerful, interconnected, and genuinely utilitarian agentic AI. However, despite MCP's growing adoption, existing benchmarks often fail to capture real-world agent performance within this new paradigm, leading to a distorted perception of their true operational value and an inability to reliably differentiate proficiencies. To bridge this critical evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark specifically engineered to rigorously assess language agent capabilities in MCP-mediated tool interactions. Core contributions of MCP-AgentBench include: the establishment of a robust MCP testbed comprising 33 operational servers with 188 distinct tools; the development of a benchmark featuring 600 systematically designed queries distributed across 6 distinct categories of varying interaction complexity; and the introduction of MCP-Eval, a novel outcome-oriented evaluation methodology prioritizing real-world task success. Through extensive empirical evaluation of leading language agents, we provide foundational insights. MCP-AgentBench aims to equip the research community with a standardized and reliable framework to build, validate, and advance agents capable of fully leveraging MCP's transformative benefits, thereby accelerating progress toward truly capable and interoperable AI systems.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Guo, Zikang and Xu, Benfeng and Zhu, Chiwei and Hong, Wentao and Wang, Xiaorui and Mao, Zhendong},
	month = sep,
	year = {2025},
	note = {arXiv:2509.09734 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{wu_mcpmark_2025,
	title = {{MCPMark}: {A} {Benchmark} for {Stress}-{Testing} {Realistic} and {Comprehensive} {MCP} {Use}},
	shorttitle = {{MCPMark}},
	url = {http://arxiv.org/abs/2509.24002},
	doi = {10.48550/arXiv.2509.24002},
	abstract = {MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of \$127\$ high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only \$52.56\${\textbackslash}\% pass@1 and \$33.86\${\textbackslash}\% pass{\textasciicircum}4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below \$30\${\textbackslash}\% pass@1 and \$15\${\textbackslash}\% pass{\textasciicircum}4. On average, LLMs require \$16.2\$ execution turns and \$17.4\$ tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Wu, Zijian and Liu, Xiangyan and Zhang, Xinyuan and Chen, Lingjun and Meng, Fanqing and Du, Lingxiao and Zhao, Yiran and Zhang, Fanshi and Ye, Yaoqi and Wang, Jiawei and Wang, Zirui and Ni, Jinjie and Yang, Yufan and Xu, Arvin and Shieh, Michael Qizhe},
	month = sep,
	year = {2025},
	note = {arXiv:2509.24002 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{luo_evaluation_2025,
	title = {Evaluation {Report} on {MCP} {Servers}},
	url = {http://arxiv.org/abs/2504.11094},
	doi = {10.48550/arXiv.2504.11094},
	abstract = {With the rise of LLMs, a large number of Model Context Protocol (MCP) services have emerged since the end of 2024. However, the effectiveness and efficiency of MCP servers have not been well studied. To study these questions, we propose an evaluation framework, called MCPBench. We selected several widely used MCP server and conducted an experimental evaluation on their accuracy, time, and token usage. Our experiments showed that the most effective MCP, Bing Web Search, achieved an accuracy of 64\%. Importantly, we found that the accuracy of MCP servers can be substantially enhanced by involving declarative interface. This research paves the way for further investigations into optimized MCP implementations, ultimately leading to better AI-driven applications and data retrieval solutions.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Luo, Zhiling and Shi, Xiaorong and Lin, Xuanrui and Gao, Jinyang},
	month = apr,
	year = {2025},
	note = {arXiv:2504.11094 [cs]},
	keywords = {Computer Science - Databases, Computer Science - Information Retrieval},
}

@misc{wang_mcp-bench_2025,
	title = {{MCP}-{Bench}: {Benchmarking} {Tool}-{Using} {LLM} {Agents} with {Complex} {Real}-{World} {Tasks} via {MCP} {Servers}},
	shorttitle = {{MCP}-{Bench}},
	url = {http://arxiv.org/abs/2508.20453},
	doi = {10.48550/arXiv.2508.20453},
	abstract = {We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Wang, Zhenting and Chang, Qi and Patel, Hemani and Biju, Shashank and Wu, Cheng-En and Liu, Quan and Ding, Aolin and Rezazadeh, Alireza and Shah, Ankit and Bao, Yujia and Siow, Eugene},
	month = aug,
	year = {2025},
	note = {arXiv:2508.20453 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{noauthor_badlogicpi-mono_nodate,
	title = {badlogic/pi-mono: {AI} agent toolkit: coding agent {CLI}, unified {LLM} {API}, {TUI} \& web {UI} libraries, {Slack} bot, {vLLM} pods},
	url = {https://github.com/badlogic/pi-mono},
	urldate = {2026-01-29},
}

@misc{noauthor_mcpserversazuremcpserver_nodate,
	title = {mcp/servers/{Azure}.{Mcp}.{Server} at main · microsoft/mcp},
	url = {https://github.com/microsoft/mcp/tree/main/servers/Azure.Mcp.Server},
	abstract = {Catalog of official Microsoft MCP (Model Context Protocol) server implementations for AI-powered data access and tool integration - microsoft/mcp},
	language = {en},
	urldate = {2026-01-29},
	journal = {GitHub},
}

@misc{hobin_hhobindataiku_factory_2025,
	title = {hhobin/dataiku\_factory},
	url = {https://github.com/hhobin/dataiku_factory},
	abstract = {Dataiku MCP tools},
	urldate = {2026-01-29},
	author = {Hobin, Henry},
	month = dec,
	year = {2025},
	note = {original-date: 2025-07-08T22:04:51Z},
}

@misc{noauthor_ibmmcp_2026,
	title = {{IBM}/mcp},
	copyright = {Apache-2.0},
	url = {https://github.com/IBM/mcp},
	abstract = {A collection of Model Context Protocol (MCP) servers, clients and developer tools by IBM.},
	urldate = {2026-01-29},
	publisher = {International Business Machines},
	month = jan,
	year = {2026},
	note = {original-date: 2025-04-02T23:35:03Z},
	keywords = {agents, llm, mcp, modelcontextprotocol, toolcalling, tools},
}

@misc{noauthor_h2oaih2ogpte-mcp-server_nodate,
	title = {h2oai/h2ogpte-mcp-server},
	url = {https://github.com/h2oai/h2ogpte-mcp-server},
	urldate = {2026-01-29},
}

@misc{noauthor_building_nodate,
	title = {Building an {AI} {Agent} for {Altair} {HyperMesh} using {LangChain} and {OpenAI} {\textbar} {Roland} {Jones} posted on the topic {\textbar} {LinkedIn}},
	url = {https://www.linkedin.com/posts/roland-m-jones_onlyforward-aiagents-cae-activity-7360709095764422656-LMUN},
	abstract = {We all hear the hype about AI Agents, and how they're going to make tools so much easier to use and more accessible. I thought I'd take a crack at building an assistant for Altair HyperMesh using LangChain and OpenAI on Microsoft Azure, exposed as a Streamlit application. Take a look at this short {\textasciitilde}2min video, and see how this simple agent can:

- query model information
- perform actions such as meshing my components
- setup an OptiStruct Analysis
- run OptiStruct in the background

The approach I've taken is highly extensible, so you can continue to add as much functionality as you'd like to enable.

Keen to hear your feedback, and what you think I should add next!

\#onlyforward \#aiagents \#cae {\textbar} 18 comments on LinkedIn},
	language = {en},
	urldate = {2026-01-29},
}

@misc{noauthor_dominodatalabdomino_mcp_server_2026,
	title = {dominodatalab/domino\_mcp\_server},
	url = {https://github.com/dominodatalab/domino_mcp_server},
	urldate = {2026-01-29},
	publisher = {Domino Data Lab},
	month = jan,
	year = {2026},
	note = {original-date: 2025-06-25T21:30:09Z},
}

@misc{noauthor_model_2026,
	title = {Model {Context} {Protocol} ({MCP}) on {Databricks} {\textbar} {Databricks} on {AWS}},
	url = {https://docs.databricks.com/aws/en/generative-ai/mcp/},
	abstract = {Use Model Context Protocol (MCP) on Databricks to connect AI agents with tools, data, and workflows through a standardized, secure interface.},
	language = {en},
	urldate = {2026-01-29},
	month = jan,
	year = {2026},
}

@misc{noauthor_linux_nodate,
	title = {Linux {Foundation} {Announces} the {Formation} of the {Agentic} {AI} {Foundation} ({AAIF}), {Anchored} by {New} {Project} {Contributions} {Including} {Model} {Context} {Protocol} ({MCP}), goose and {AGENTS}.md},
	url = {https://www.linuxfoundation.org/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation},
	abstract = {Linux Foundation Announces the Formation of the Agentic AI Foundation},
	language = {en},
	urldate = {2026-01-29},
}

@misc{noauthor_mcpexampleslaunchmybakery_nodate,
	title = {mcp/examples/launchmybakery at main · google/mcp},
	url = {https://github.com/google/mcp/tree/main/examples/launchmybakery},
	abstract = {Google 💚 MCP. Contribute to google/mcp development by creating an account on GitHub.},
	language = {en},
	urldate = {2026-01-29},
	journal = {GitHub},
}

@misc{noauthor_googleapisgenai-toolbox_2026,
	title = {googleapis/genai-toolbox},
	copyright = {Apache-2.0},
	url = {https://github.com/googleapis/genai-toolbox},
	abstract = {MCP Toolbox for Databases is an open source MCP server for databases.},
	urldate = {2026-01-29},
	publisher = {Google APIs},
	month = jan,
	year = {2026},
	note = {original-date: 2024-06-07T20:52:54Z},
	keywords = {agent, agents, ai, bigquery, clickhouse, database, elasticsearch, firestore, genai, llm, mcp, mongodb, mysql, oracle, postgresql, redis, server, spanner, tidb},
}

@misc{noauthor_datarobot-communitydatarobot-mcp-template_2026,
	title = {datarobot-community/datarobot-mcp-template},
	copyright = {Apache-2.0},
	url = {https://github.com/datarobot-community/datarobot-mcp-template},
	abstract = {DataRobot MCP template for building Model Context Protocol servers with deep DataRobot integration.},
	urldate = {2026-01-29},
	publisher = {DataRobot Community Repositories},
	month = jan,
	year = {2026},
	note = {original-date: 2025-10-15T11:42:08Z},
	keywords = {datarobot, mcp},
}

@misc{noauthor_sassoftwarerestaf-demos_nodate,
	title = {sassoftware/restaf-demos at mcp-serverjs},
	url = {https://github.com/sassoftware/restaf-demos},
	abstract = {A collection of examples using restaf in a nodejs environment. - sassoftware/restaf-demos at mcp-serverjs},
	language = {en},
	urldate = {2026-01-29},
	journal = {GitHub},
}

@misc{noauthor_gartner_nodate,
	title = {Gartner 2025 {Magic} {Quadrant} for {Data} {Science} and {ML} {Platforms}},
	url = {https://cloud.google.com/blog/products/ai-machine-learning/gartner-2025-magic-quadrant-for-data-science-and-ml-platforms},
	abstract = {Google is a Leader in Gartner’s 2025 Magic Quadrant for Data Science and Machine Learning Platforms, validating investments in predictive and gen AI.},
	language = {en},
	urldate = {2026-01-29},
	journal = {Google Cloud Blog},
}

@misc{noauthor_magic_nodate,
	title = {Magic {Quadrant} for {Data} {Science} and {Machine} {Learning} {Platforms}},
	url = {https://www.gartner.com/en/documents/6533902},
	abstract = {Gartner Magic Quadrant for Data Science and Machine Learning Platforms. A graphical competitive positioning of Leaders, Visionaires, Niche Players and Challengers for Data Science and Machine Learning Platforms},
	language = {en},
	urldate = {2026-01-29},
	journal = {Gartner},
}

@misc{noauthor_clouderaiceberg-mcp-server_2026,
	title = {cloudera/iceberg-mcp-server},
	copyright = {Apache-2.0},
	url = {https://github.com/cloudera/iceberg-mcp-server},
	urldate = {2026-01-29},
	publisher = {Cloudera},
	month = jan,
	year = {2026},
	note = {original-date: 2025-04-15T07:49:30Z},
}

@misc{bakakeu_jupiterbakayx-mcp-wrapper_2026,
	title = {jupiterbak/{AYX}-{MCP}-{Wrapper}},
	url = {https://github.com/jupiterbak/AYX-MCP-Wrapper},
	abstract = {A Model Context Protocol (MCP) server that provides a comprehensive interface to Alteryx Servers.},
	urldate = {2026-01-29},
	author = {Bakakeu, Jupiter},
	month = jan,
	year = {2026},
	note = {original-date: 2025-06-20T07:37:56Z},
}

@misc{noauthor_matlab_nodate,
	title = {{MATLAB} {MCP} {Core} {Server}},
	url = {https://www.mathworks.com/products/matlab-mcp-core-server.html},
	abstract = {MATLAB MCP Core Server enables your AI applications to start and quit MATLAB, write and run MATLAB code, and assess MATLAB code for style and correctness.},
	language = {en},
	urldate = {2026-01-29},
}

@misc{noauthor_snowflake-managed_nodate,
	title = {Snowflake-managed {MCP} server {\textbar} {Snowflake} {Documentation}},
	url = {https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-agents-mcp},
	urldate = {2026-01-29},
}

@misc{noauthor_amazon_nodate,
	title = {Amazon {SageMaker} {AI} {MCP} {Server} {\textbar} {AWS} {MCP} {Servers}},
	url = {https://awslabs.github.io/mcp/servers/sagemaker-ai-mcp-server},
	abstract = {\{\vphantom{\}}`},
	language = {en},
	urldate = {2026-01-29},
}

@misc{software_we_2025,
	title = {We removed 80\% of our agent’s tools},
	url = {https://vercel.com/blog/we-removed-80-percent-of-our-agents-tools},
	abstract = {We spent months building a sophisticated text-to-sql agent, but as it turns out, sometimes simpler is better. Giving it the ability to execute arbitrary bash commands outperformed everything we built. We call this a file system agent.},
	language = {en},
	urldate = {2026-01-29},
	journal = {Vercel},
	author = {Software, Andrew Qu Chief of and {Vercel}},
	month = dec,
	year = {2025},
}

@article{chakraborty_beyond_2025,
	title = {Beyond {ETL}: {How} {AI} {Agents} {Are} {Building} {Self}-{Healing} {Data} {Pipelines}},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2709-104X},
	shorttitle = {Beyond {ETL}},
	url = {https://al-kindipublishers.org/index.php/jcsts/article/view/9427},
	doi = {10.32996/jcsts.2025.7.3.81},
	abstract = {This article explores the transformative role of artificial intelligence agents in modernizing traditional Extract, Transform, Load (ETL) processes through the development of self-healing data pipelines. As organizations face increasing data complexity and volume, conventional ETL workflows with their reactive problem-solving approaches, limited scalability, and resource-intensive maintenance requirements are proving inadequate. The article examines how AI-powered agents, operating in a layered architecture of horizontal (cross-pipeline) and vertical (domain-specific) intelligences, revolutionize data management through proactive issue detection, autonomous remediation, and continuous learning capabilities. These intelligent systems can detect subtle anomalies before they become critical failures, implement fixes without human intervention, and continuously improve through feedback loops. The article further investigates how AI simplifies both data and metadata extraction through adaptive connectors, format recognition, and automated metadata management. Drawing on industry case studies and research, the article documents significant operational benefits and strategic advantages realized by organizations implementing these technologies, including reduced downtime, engineering efficiency, data trustworthiness, and regulatory compliance. Finally, the article looks ahead to emerging capabilities like cognitive pipelines, natural language interfaces, cross-organizational intelligence, and predictive infrastructure scaling that will define the future evolution of data management.},
	language = {en},
	number = {3},
	urldate = {2026-01-29},
	journal = {Journal of Computer Science and Technology Studies},
	author = {Chakraborty, Soumen},
	month = may,
	year = {2025},
	keywords = {Anomaly detection, Artificial intelligence agents, ETL automation, Metadata management, Self-healing data pipelines},
	pages = {741--756},
}

@misc{mahendra_ethical_2025,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Ethical {AI} and {Data} {Engineering}: {Building} {Transparent} and {Accountable} {Systems} - {A} {Systematic} {Review}},
	shorttitle = {Ethical {AI} and {Data} {Engineering}},
	url = {https://papers.ssrn.com/abstract=5237146},
	doi = {10.2139/ssrn.5237146},
	abstract = {We conducted a systematic review of 19 peerreviewed articles focused on the key developments in ethical artificial intelligence and ethical data engineering spanning 2021 and previewing 2025. We review new frameworks, initiatives, and technologies that are being developed to enhance the transparency and accountability of AI systems. We find evidence of a move from algorithmic ethics to a data-centric paradigm, complemented by increasing attention to metrics of fairness and explainability. This enables steps towards bridging the gaps between practice and ethical theory requirements as well as in cross-cultural and small-scale deployments, despite progress in the development of technical solutions. The review identifies differences in ethical demand across contexts and proposes that future work on ethical demand be directed toward longitudinal outcomes, forms of stakeholder engagement, and adherence to shifting regulatory demands. In sum, we present an integrated approach that addresses ethical considerations across the data engineering life cycle.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {Social Science Research Network},
	author = {Mahendra, Prateik},
	month = apr,
	year = {2025},
	keywords = {AI Transparency, Algorithmic Accountability, Data-centric Ethics, Ethical Data Engineering, Fairness Metrics},
}

@article{hong_next-generation_2025,
	title = {Next-{Generation} {Database} {Interfaces}: {A} {Survey} of {LLM}-{Based} {Text}-to-{SQL}},
	volume = {37},
	issn = {1558-2191},
	shorttitle = {Next-{Generation} {Database} {Interfaces}},
	url = {https://ieeexplore.ieee.org/abstract/document/11160657},
	doi = {10.1109/TKDE.2025.3609486},
	abstract = {Generating accurate SQL from users’ natural language questions (text-to-SQL) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and SQL generation. Traditional text-to-SQL systems, which combine human engineering and deep neural networks, have made significant progress. Subsequently, pre-trained language models (PLMs) have been developed for text-to-SQL tasks, achieving promising results. However, as modern databases and user questions grow more complex, PLMs with a limited parameter size often produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods, which restrict the application of PLM-based systems. Recently, large language models (LLMs) have shown significant capabilities in natural language understanding as model scale increases. Thus, integrating LLM-based solutions can bring unique opportunities, improvements, and solutions to text-to-SQL research. In this survey, we provide a comprehensive review of existing LLM-based text-to-SQL studies. Specifically, we offer a brief overview of the technical challenges and evolutionary process of text-to-SQL. Next, we introduce the datasets and metrics designed to evaluate text-to-SQL systems. Subsequently, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we make a summary and discuss the remaining challenges in this field and suggest expectations for future research directions.},
	number = {12},
	urldate = {2026-01-29},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Hong, Zijin and Yuan, Zheng and Zhang, Qinggang and Chen, Hao and Dong, Junnan and Huang, Feiran and Huang, Xiao},
	month = dec,
	year = {2025},
	keywords = {Accuracy, Complexity theory, Databases, Large language models, Measurement, Natural language processing, Structured Query Language, Surveys, Taxonomy, Text-to-SQL, Training, database, large language models, natural language understanding},
	pages = {7328--7345},
}

@inproceedings{zhang_data_2025,
	title = {Data {Cleaning} {Using} {Large} {Language} {Models}},
	issn = {2473-3490},
	url = {https://ieeexplore.ieee.org/abstract/document/11107465},
	doi = {10.1109/ICDEW67478.2025.00008},
	abstract = {Data cleaning is a crucial yet challenging task in data analysis, often requiring significant manual effort. To automate data cleaning, previous systems have relied on statistical rules derived from erroneous data, which typically result in low accuracy and recall. This work introduces Cocoon, a novel data cleaning system that combines statistical error detection and correction with semantic understanding by leveraging large language models. However, data cleaning remains too complex for current LLMs to handle in a single step. To address this, Cocoon decomposes complex cleaning tasks into manageable components, following a workflow that mimics human cleaning processes. Our experiments demonstrate that Cocoon outperforms state-of-the-art data cleaning systems on standard benchmarks.},
	urldate = {2026-01-29},
	booktitle = {2025 {IEEE} 41st {International} {Conference} on {Data} {Engineering} {Workshops} ({ICDEW})},
	author = {Zhang, Shuo and Huang, Zezhou and Wu, Eugene},
	month = may,
	year = {2025},
	note = {ISSN: 2473-3490},
	keywords = {Benchmark testing, Cleaning, Compound AI systems, Data Cleaning, Data analysis, Data engineering, Large Language Model, Large language models, MIMICs, Manuals, Prompt Engineering, Prompt engineering, Semantics, Standards},
	pages = {28--32},
}

@article{liu_survey_2025,
	title = {A {Survey} of {Text}-to-{SQL} in the {Era} of {LLMs}: {Where} {Are} {We}, and {Where} {Are} {We} {Going}?},
	volume = {37},
	issn = {1558-2191},
	shorttitle = {A {Survey} of {Text}-to-{SQL} in the {Era} of {LLMs}},
	url = {https://ieeexplore.ieee.org/abstract/document/11095853},
	doi = {10.1109/TKDE.2025.3592032},
	abstract = {Translating users’ natural language queries (NL) into SQL queries (i.e., Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of Text-to-SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: Text-to-SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks; (3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL solutions. Finally, we discuss the research challenges and open problems of Text-to-SQL in the LLMs era.},
	number = {10},
	urldate = {2026-01-29},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xinyu and Shen, Shuyu and Li, Boyan and Ma, Peixian and Jiang, Runzhi and Zhang, Yuxin and Fan, Ju and Li, Guoliang and Tang, Nan and Luo, Yuyu},
	month = oct,
	year = {2025},
	keywords = {Analytical models, Benchmark testing, Data models, Databases, Error analysis, Natural language to SQL, Reviews, Structured Query Language, Surveys, Taxonomy, Training data, database interface, large language models, text-to-SQL},
	pages = {5735--5754},
}

@inproceedings{yang_generating_2025,
	title = {Generating {Data} {Engineering} {Code} {Using} {Llms}},
	url = {https://ieeexplore.ieee.org/abstract/document/11344354},
	doi = {10.1109/CASCON66301.2025.00036},
	abstract = {Data engineering is a complex and time-consuming part of data science, critical for transforming raw data into actionable insights. This complexity stems from diverse and large data sources, data-dependent logic, and exploratory workflows. We perform an empirical evaluation of the performance of three LLMs (GPT-4o-mini, Claude-3.5-Haiku, and Gemini-2.0-Flash), which have been shown to be effective at code generation, on multi-step data engineering notebooks. Our study considers the impact of prompting with previous execution context, output samples, and the application of iterative refinement on entire notebooks and their individual steps. We benchmark performance against the ARCADE dataset and introduce a new benchmark derived from Spider 2.0 (Spider2-intents) to mitigate potential data leakage. Our results show that LLMs generate syntactically and semantically correct code, with output data match scores reaching up to 80\%, and BLEU scores of 0.35 on our newly created Spider2-intents benchmark. While generated code trends toward reduced runtime, memory, and CPU usage, these improvements are not statistically significant. Further analysis reveals that ambiguity in user intents is the leading cause of functional correctness issues, accounting for 40.74\% of such cases. We also observe that iterative refinement shows a modest but statistically inconclusive trend toward improved output correctness, with gains of up to 2.9\% after two rounds of notebook and intent refinement each.},
	urldate = {2026-01-29},
	booktitle = {2025 {IEEE} {International} {Conference} on {Collaborative} {Advances} in {Software} and {COmputiNg} ({CASCON})},
	author = {Yang, Jialin and Maciszewski, Bart and Owolabi, Saviour and Abdellatif, Ahmad and Leung, Henry and Drew, Steve},
	month = nov,
	year = {2025},
	keywords = {Benchmark testing, Code Generation, Codes, Data Engineering, Data engineering, Impedance matching, Iterative methods, Large Language Models (LLMs), Market research, Multi-step Data Transformation, Optimization, Runtime, Soft sensors, Transforms},
	pages = {142--151},
}

@misc{kandogan_orchestrating_2025,
	title = {Orchestrating {Agents} and {Data} for {Enterprise}: {A} {Blueprint} {Architecture} for {Compound} {AI}},
	shorttitle = {Orchestrating {Agents} and {Data} for {Enterprise}},
	url = {http://arxiv.org/abs/2504.08148},
	doi = {10.48550/arXiv.2504.08148},
	abstract = {Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company proprietary data, models, and APIs, and meeting cost, quality, responsiveness, and other requirements. To address these challenges, there is a notable shift from monolithic models to compound AI systems, with the premise of more powerful, versatile, and reliable applications. However, progress thus far has been piecemeal, with proposals for agentic workflows, programming models, and extended LLM capabilities, without a clear vision of an overall architecture. In this paper, we propose a 'blueprint architecture' for compound AI systems for orchestrating agents and data for enterprise applications. In our proposed architecture the key orchestration concept is 'streams' to coordinate the flow of data and instructions among agents. Existing proprietary models and APIs in the enterprise are mapped to 'agents', defined in an 'agent registry' that serves agent metadata and learned representations for search and planning. Agents can utilize proprietary data through a 'data registry' that similarly registers enterprise data of various modalities. Tying it all together, data and task 'planners' break down, map, and optimize tasks and queries for given quality of service (QoS) requirements such as cost, accuracy, and latency. We illustrate an implementation of the architecture for a use-case in the HR domain and discuss opportunities and challenges for 'agentic AI' in the enterprise.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Kandogan, Eser and Bhutani, Nikita and Zhang, Dan and Chen, Rafael Li and Gurajada, Sairam and Hruschka, Estevam},
	month = apr,
	year = {2025},
	note = {arXiv:2504.08148 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{rahman_llm-based_2025,
	title = {{LLM}-{Based} {Data} {Science} {Agents}: {A} {Survey} of {Capabilities}, {Challenges}, and {Future} {Directions}},
	shorttitle = {{LLM}-{Based} {Data} {Science} {Agents}},
	url = {http://arxiv.org/abs/2510.04023},
	doi = {10.48550/arXiv.2510.04023},
	abstract = {Recent advances in large language models (LLMs) have enabled a new class of AI agents that automate multiple stages of the data science workflow by integrating planning, tool use, and multimodal reasoning across text, code, tables, and visuals. This survey presents the first comprehensive, lifecycle-aligned taxonomy of data science agents, systematically analyzing and mapping forty-five systems onto the six stages of the end-to-end data science process: business understanding and data acquisition, exploratory analysis and visualization, feature engineering, model building and selection, interpretation and explanation, and deployment and monitoring. In addition to lifecycle coverage, we annotate each agent along five cross-cutting design dimensions: reasoning and planning style, modality integration, tool orchestration depth, learning and alignment methods, and trust, safety, and governance mechanisms. Beyond classification, we provide a critical synthesis of agent capabilities, highlight strengths and limitations at each stage, and review emerging benchmarks and evaluation practices. Our analysis identifies three key trends: most systems emphasize exploratory analysis, visualization, and modeling while neglecting business understanding, deployment, and monitoring; multimodal reasoning and tool orchestration remain unresolved challenges; and over 90\% lack explicit trust and safety mechanisms. We conclude by outlining open challenges in alignment stability, explainability, governance, and robust evaluation frameworks, and propose future research directions to guide the development of robust, trustworthy, low-latency, transparent, and broadly accessible data science agents.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Rahman, Mizanur and Bhuiyan, Amran and Islam, Mohammed Saidul and Laskar, Md Tahmid Rahman and Mahbub, Ridwan and Masry, Ahmed and Joty, Shafiq and Hoque, Enamul},
	month = oct,
	year = {2025},
	note = {arXiv:2510.04023 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{sambath_narayanan_ai-driven_2025,
	title = {{AI}-{Driven} {Data} {Engineering} {Workflows} for {Dynamic} {ETL} {Optimization} in {Cloud}-{Native} {Data} {Analytics} {Ecosystems}},
	volume = {7},
	issn = {31175481},
	url = {https://aijcst.org/index.php/aijcst/article/view/121},
	doi = {10.63282/3117-5481/AIJCST-V7I3P108},
	urldate = {2026-01-29},
	journal = {American International Journal of Computer Science and Technology},
	author = {Sambath Narayanan, Dinesh Babu Govindarajulunaidu},
	year = {2025},
}

@incollection{ranjan_ai_2025,
	address = {Dordrecht},
	title = {{AI} and {Optimization}: {Transforming} {Data} {Engineering} {Applications}},
	volume = {196},
	isbn = {978-94-6463-786-1 978-94-6463-787-8},
	shorttitle = {{AI} and {Optimization}},
	url = {https://www.atlantis-press.com/doi/10.2991/978-94-6463-787-8_52},
	doi = {10.2991/978-94-6463-787-8_52},
	language = {en},
	urldate = {2026-01-29},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Artificial} {Intelligence} for {Sustainable} {Development} ({RAISD} 2025)},
	publisher = {Atlantis Press International BV},
	author = {Jonnalagadda, Anil Kumar and Dutta, Kailash Pati and Ranjan, Piyush and Myakala, Praveen Kumar},
	editor = {Ranjan, Piyush and Pandey, Sumit Kumar and Dutta, Kailash Pati and Alam, Md. Irfan},
	year = {2025},
	note = {Series Title: Advances in Intelligent Systems Research},
	pages = {686--702},
}

@misc{sun_data_2025,
	title = {Data {Agent}: {A} {Holistic} {Architecture} for {Orchestrating} {Data}+{AI} {Ecosystems}},
	shorttitle = {Data {Agent}},
	url = {http://arxiv.org/abs/2507.01599},
	doi = {10.48550/arXiv.2507.01599},
	abstract = {Traditional Data+AI systems utilize data-driven techniques to optimize performance, but they rely heavily on human experts to orchestrate system pipelines, enabling them to adapt to changes in data, queries, tasks, and environments. For instance, while there are numerous data science tools available, developing a pipeline planning system to coordinate these tools remains challenging. This difficulty arises because existing Data+AI systems have limited capabilities in semantic understanding, reasoning, and planning. Fortunately, we have witnessed the success of large language models (LLMs) in enhancing semantic understanding, reasoning, and planning abilities. It is crucial to incorporate LLM techniques to revolutionize data systems for orchestrating Data+AI applications effectively. To achieve this, we propose the concept of a 'Data Agent' - a comprehensive architecture designed to orchestrate Data+AI ecosystems, which focuses on tackling data-related tasks by integrating knowledge comprehension, reasoning, and planning capabilities. We delve into the challenges involved in designing data agents, such as understanding data/queries/environments/tools, orchestrating pipelines/workflows, optimizing and executing pipelines, and fostering pipeline self-reflection. Furthermore, we present examples of data agent systems, including a data science agent, data analytics agents (such as unstructured data analytics agent, semantic structured data analytics agent, data lake analytics agent, and multi-modal data analytics agent), and a database administrator (DBA) agent. We also outline several open challenges associated with designing data agent systems.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Sun, Zhaoyan and Wang, Jiayi and Zhao, Xinyang and Wang, Jiachi and Li, Guoliang},
	month = jul,
	year = {2025},
	note = {arXiv:2507.01599 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Machine Learning},
}

@article{quang_hai_khuat_leveraging_2025,
	title = {Leveraging {Generative} {AI} for {Data} {Engineering} {Workflows}},
	volume = {7},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2709-104X},
	url = {https://www.al-kindipublisher.com/index.php/jcsts/article/view/9310},
	doi = {10.32996/jcsts.2025.7.3.14},
	abstract = {Generative AI represents a powerful new layer of automation for data engineering. When leveraged responsibly, it can improve efficiency, reduce errors, and even enable non-experts to contribute to data workflows, all while allowing expert data engineers to tackle more ambitious challenges. We are witnessing the early stages of this transformation. By staying informed of the latest tools, adopting best practices for AI usage, and continuously refining the human-AI partnership, data engineering teams can ride this wave to build more intelligent, adaptive, and robust data pipelines than ever before. The future data platform may very well be a co-creation of human engineers and AI, each complementing the other’s strengths – and the organizations that embrace this symbiosis will be positioned at the forefront of the data-driven era.},
	number = {3},
	urldate = {2026-01-29},
	journal = {Journal of Computer Science and Technology Studies},
	author = {{Quang Hai Khuat}},
	month = may,
	year = {2025},
	pages = {120--140},
}

@article{bhoite_autonomous_nodate,
	title = {Autonomous {AI} {Agents} for {End}-to-{End} {Data} {Engineering} {Pipelines} {Deployment}: {Enhancing} {CI}/{CD} {Pipelines}},
	shorttitle = {Autonomous {AI} {Agents} for {End}-to-{End} {Data} {Engineering} {Pipelines} {Deployment}},
	url = {https://www.authorea.com/doi/full/10.36227/techrxiv.174662424.46301311?commit=426f725a9212bfdfb8cbc88b1816729c272d1a50},
	abstract = {The integration of autonomous AI agents into data engineering workflows promises to revolutionize how data pipelines are developed, deployed, and maintained. In this work, we investigate the use of autonomous AI agents – such as AutoGPT and CrewAI-bas},
	urldate = {2026-01-29},
	author = {Bhoite, Harshraj},
}

@article{kirubakaran_governing_2025,
	title = {Governing {Cloud} {Data} {Pipelines} with {Agentic} {AI}},
	issn = {2347-8578},
	url = {http://arxiv.org/abs/2512.23737},
	doi = {10.5281/zenodo.18048728},
	abstract = {Cloud data pipelines increasingly operate under dynamic workloads, evolving schemas, cost constraints, and strict governance requirements. Despite advances in cloud-native orchestration frameworks, most production pipelines rely on static configurations and reactive operational practices, resulting in prolonged recovery times, inefficient resource utilization, and high manual overhead. This paper presents Agentic Cloud Data Engineering, a policy-aware control architecture that integrates bounded AI agents into the governance and control plane of cloud data pipelines. In Agentic Cloud Data Engineering platform, specialized agents analyze pipeline telemetry and metadata, reason over declarative cost and compliance policies, and propose constrained operational actions such as adaptive resource reconfiguration, schema reconciliation, and automated failure recovery. All agent actions are validated against governance policies to ensure predictable and auditable behavior. We evaluate Agentic Cloud Data Engineering platform using representative batch and streaming analytics workloads constructed from public enterprise-style datasets. Experimental results show that Agentic Cloud Data Engineering platform reduces mean pipeline recovery time by up to 45\%, lowers operational cost by approximately 25\%, and decreases manual intervention events by over 70\% compared to static orchestration, while maintaining data freshness and policy compliance. These results demonstrate that policy-bounded agentic control provides an effective and practical approach for governing cloud data pipelines in enterprise environments.},
	urldate = {2026-01-29},
	author = {Kirubakaran, Aswathnarayan Muthukrishnan and Parthasarathy, Adithya and Saksena, Nitin and Bodala, Ram Sekhar and Deshpande, Akshay and Malempati, Suhas and Carimireddy, Shiva and Mazumder, Abhirup},
	month = dec,
	year = {2025},
	note = {arXiv:2512.23737 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@misc{fu_autonomous_2025,
	title = {Autonomous {Data} {Agents}: {A} {New} {Opportunity} for {Smart} {Data}},
	shorttitle = {Autonomous {Data} {Agents}},
	url = {http://arxiv.org/abs/2509.18710},
	doi = {10.48550/arXiv.2509.18710},
	abstract = {As data continues to grow in scale and complexity, preparing, transforming, and analyzing it remains labor-intensive, repetitive, and difficult to scale. Since data contains knowledge and AI learns knowledge from it, the alignment between AI and data is essential. However, data is often not structured in ways that are optimal for AI utilization. Moreover, an important question arises: how much knowledge can we pack into data through intensive data operations? Autonomous data agents (DataAgents), which integrate LLM reasoning with task decomposition, action reasoning and grounding, and tool calling, can autonomously interpret data task descriptions, decompose tasks into subtasks, reason over actions, ground actions into python code or tool calling, and execute operations. Unlike traditional data management and engineering tools, DataAgents dynamically plan workflows, call powerful tools, and adapt to diverse data tasks at scale. This report argues that DataAgents represent a paradigm shift toward autonomous data-to-knowledge systems. DataAgents are capable of handling collection, integration, preprocessing, selection, transformation, reweighing, augmentation, reprogramming, repairs, and retrieval. Through these capabilities, DataAgents transform complex and unstructured data into coherent and actionable knowledge. We first examine why the convergence of agentic AI and data-to-knowledge systems has emerged as a critical trend. We then define the concept of DataAgents and discuss their architectural design, training strategies, as well as the new skills and capabilities they enable. Finally, we call for concerted efforts to advance action workflow optimization, establish open datasets and benchmark ecosystems, safeguard privacy, balance efficiency with scalability, and develop trustworthy DataAgent guardrails to prevent malicious actions.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Fu, Yanjie and Wang, Dongjie and Ying, Wangyang and Wang, Xinyuan and Zhang, Xiangliang and Liu, Huan and Pei, Jian},
	month = oct,
	year = {2025},
	note = {arXiv:2509.18710 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{noauthor_warp_nodate,
	title = {Warp: {The} {Agentic} {Development} {Environment}},
	shorttitle = {Warp},
	url = {https://www.warp.dev/},
	abstract = {The fastest way to build with multiple AI agents, from writing code to deploying it. Trusted by over half a million engineers, Warp gives developers speed, privacy, and control to ship faster.},
	language = {en-US},
	urldate = {2026-01-29},
}

@inproceedings{v_scaling_2025-1,
	title = {Scaling {Multi}-{MCP} {AI} {Agents} {Beyond} {Context} {Limits}},
	issn = {2767-1097},
	url = {https://ieeexplore.ieee.org/document/11294866},
	doi = {10.1109/CSITSS67709.2025.11294866},
	abstract = {Transformer models cannot reason over unbounded interactions; every request must fit within a finite context window. As AI agents evolve into tool-augmented systems exposing dozens of Model Context Protocol (MCP) servers-Google Workspace, Notion, Git, Slack and bespoke micro-services-the cumulative token footprint routinely exceeds the 128 K-200 K boundaries of state-of-the-art LLMs. This paper delivers the first large-scale mixed-methods study of strategies that reconcile multi-MCP autonomy with context limits. We benchmark six optimisation families (baseline, RAG-MCP, contextual compression, hierarchical MCP routing, adaptive chunking and multi-agent coordination) across quantitative metrics and eight qualitative case studies. Experiments on TaskBench, MCP-Radar, MCPEval and BFCL show mean token reduction of 37\%, mean accuracy gain 23 pp. We synthesize a decision matrix, propose a governance framework for enterprise deployments. Results position contextual-compression plus RAG-MCP as the current Pareto frontier for developers.},
	urldate = {2026-01-29},
	booktitle = {2025 9th {International} {Conference} on {Computational} {System} and {Information} {Technology} for {Sustainable} {Solutions} ({CSITSS})},
	author = {V, Kiran and Prajapati, Vijay Kumar},
	month = nov,
	year = {2025},
	note = {ISSN: 2767-1097},
	keywords = {Accuracy, Context modeling, LLM constraints, Model Context Protocol, Multi-agent systems, Optimization, Reliability engineering, Retrieval augmented generation, Robustness, Routing, Routing protocols, Scalability, Transformers, agent architectures, context window optimization, retrieval-augmented generation, tool augmentation},
	pages = {1--6},
}

@misc{ray_survey_2025,
	title = {A {Survey} on {Model} {Context} {Protocol}: {Architecture}, {State}-of-the-art, {Challenges} and {Future} {Directions}},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	shorttitle = {A {Survey} on {Model} {Context} {Protocol}},
	url = {https://www.techrxiv.org/users/913189/articles/1286748-a-survey-on-model-context-protocol-architecture-state-of-the-art-challenges-and-future-directions?commit=9234df64ba662b5f79543305ebc7334fbf834043},
	doi = {10.36227/techrxiv.174495492.22752319/v1},
	urldate = {2026-01-29},
	publisher = {Preprints},
	author = {Ray, Partha Pratim},
	month = apr,
	year = {2025},
}

@misc{noauthor_code_2025,
	title = {Code {Mode}: the better way to use {MCP}},
	shorttitle = {Code {Mode}},
	url = {https://blog.cloudflare.com/code-mode/},
	abstract = {It turns out we've all been using MCP wrong. Most agents today use MCP by exposing the "tools" directly to the LLM. We tried something different: Convert the MCP tools into a TypeScript API, and then ask an LLM to write code that calls that API. The results are striking.},
	language = {en},
	urldate = {2026-01-29},
	journal = {The Cloudflare Blog},
	month = sep,
	year = {2025},
}

@misc{luo_mcp-universe_2025,
	title = {{MCP}-{Universe}: {Benchmarking} {Large} {Language} {Models} with {Real}-{World} {Model} {Context} {Protocol} {Servers}},
	shorttitle = {{MCP}-{Universe}},
	url = {http://arxiv.org/abs/2508.14704},
	doi = {10.48550/arXiv.2508.14704},
	abstract = {The Model Context Protocol (MCP) has emerged as a transformative standard for connecting large language models (LLMs) to external data sources and tools, rapidly gaining adoption across major AI providers and development platforms. However, existing benchmarks are overly simplistic and fail to capture real application challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To address this critical gap, we introduce MCP-Universe, the first comprehensive benchmark specifically designed to evaluate LLMs in realistic and hard tasks through interaction with real-world MCP servers. Our benchmark encompasses 6 core domains spanning 11 different MCP servers: Location Navigation, Repository Management, Financial Analysis, 3D Design, Browser Automation, and Web Searching. To ensure rigorous evaluation, we implement executionbased evaluators, including format evaluators for agent format compliance, static evaluators for timeinvariant content matching, and dynamic evaluators that automatically retrieve real-time ground truth for temporally sensitive tasks. Through extensive evaluation of leading LLMs, we find that even top-performing models such as GPT-5 (43.72\% success rate), Grok-4 (33.33\% success rate) and Claude-4.0-Sonnet (29.44\% success rate) exhibit significant performance limitations. In addition, our benchmark poses a significant long-context challenge for LLM agents, as the number of input tokens increases rapidly with the number of interaction steps. Moreover, it introduces an unknowntools challenge, as LLM agents often lack familiarity with the precise usage of the MCP servers. Notably, enterprise-level agents like Cursor cannot achieve better performance than standard ReAct frameworks. Beyond evaluation, we open-source our extensible evaluation framework with UI support, enabling researchers and practitioners to seamlessly integrate new agents and MCP servers while fostering innovation in the rapidly evolving MCP ecosystem.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Luo, Ziyang and Shen, Zhiqi and Yang, Wenzhuo and Zhao, Zirui and Jwalapuram, Prathyusha and Saha, Amrita and Sahoo, Doyen and Savarese, Silvio and Xiong, Caiming and Li, Junnan},
	month = aug,
	year = {2025},
	note = {arXiv:2508.14704 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{noauthor_context7_nodate,
	title = {Context7 - {Up}-to-date documentation for {LLMs} and {AI} code editors},
	url = {https://context7.com},
	abstract = {Generate context with up-to-date documentation for LLMs and AI code editors},
	language = {en},
	urldate = {2026-01-29},
	journal = {Context7},
}

@misc{noauthor_exploring_nodate,
	title = {Exploring the {Data} {Engineering} {Agent} in {BigQuery}},
	url = {https://cloud.google.com/blog/products/data-analytics/exploring-the-data-engineering-agent-in-bigquery},
	abstract = {The Data Engineering Agent in BigQuery, now in preview, automates many tedious development, maintenance, and troubleshooting tasks.},
	language = {en},
	urldate = {2026-01-29},
	journal = {Google Cloud Blog},
}

@misc{zhai_toolcaching_2026,
	title = {{ToolCaching}: {Towards} {Efficient} {Caching} for {LLM} {Tool}-calling},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{ToolCaching}},
	url = {https://arxiv.org/abs/2601.15335},
	doi = {10.48550/ARXIV.2601.15335},
	abstract = {Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11\% higher cache hit ratios and 34\% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Zhai, Yi and Shen, Dian and Luo, Junzhou and Yang, Bin},
	year = {2026},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences, I.2; H.4, Programming Languages (cs.PL), Software Engineering (cs.SE)},
}

@article{venkiteela_enterprise_2026,
	title = {An {Enterprise} {Agentic} {Architecture} {Framework} for {Agentic} {AI} {Governance} and {Scalable} {Autonomy}},
	volume = {2},
	issn = {3110-3170},
	url = {https://journal.futuristech.co.id/index.php/sjcs/article/view/368},
	doi = {10.64539/sjcs.v2i1.2026.368},
	abstract = {The rise of agentic artificial intelligence is changing how businesses operate, manage systems, and oversee digital workflows. These systems are different from normal automation or standalone AI models because they rely on structured thinking secure tool usage advanced teamwork between multiple agents, and ongoing feedback in complex environments with hybrid and multi-cloud systems. But there is a major issue businesses don’t have a clear framework to and use and expand agentic AI while staying compliant. This document tackles that problem by presenting the Enterprise Agentic Architecture Framework. This is a detailed multi-layered reference model built to help large organizations safely and use and manage agentic AI on a bigger scale. EAAF is built on six key layers: infrastructure, enterprise integration, orchestration and coordination, governance and safety, agent intelligence, and agent interaction. A central Control Plane ties all these layers together. The Control Plane plays a major role in managing policies, identity, scheduling, observability, and controlling the lifecycle of individual agents as well as multi-agent systems. Tests on real-world enterprise cases like Opportunity-to-Order automation, DevOps and AIOps pipelines, integration workflows, and collaboration across multiple agents in different domains show that EAAF improves autonomy, ensures reliable reasoning, boosts efficiency in execution, and strengthens operational resilience. Tests reveal significant boosts such as workflows running 3 to 10 times faster, cutting the average resolution time (MTTR) by 60 to 80 percent, and clear improvements in safety guided by policies. To sum up, EAAF acts as a key framework to build future enterprise AI systems. It ensures safe autonomy, sets up consistent architecture, and organizes agent-driven operations for critical tasks.},
	number = {1},
	urldate = {2026-01-29},
	journal = {Scientific Journal of Computer Science},
	author = {Venkiteela, Padmanabham},
	month = jan,
	year = {2026},
	pages = {1--17},
}

@misc{wang_deploy-master_2026,
	title = {Deploy-{Master}: {Automating} the {Deployment} of 50,000+ {Agent}-{Ready} {Scientific} {Tools} in {One} {Day}},
	shorttitle = {Deploy-{Master}},
	url = {http://arxiv.org/abs/2601.03513},
	doi = {10.48550/arXiv.2601.03513},
	abstract = {Open-source scientific software is abundant, yet most tools remain difficult to compile, configure, and reuse, sustaining a small-workshop mode of scientific computing. This deployment bottleneck limits reproducibility, large-scale evaluation, and the practical integration of scientific tools into modern AI-for-Science (AI4S) and agentic workflows. We present Deploy-Master, a one-stop agentic workflow for large-scale tool discovery, build specification inference, execution-based validation, and publication. Guided by a taxonomy spanning 90+ scientific and engineering domains, our discovery stage starts from a recall-oriented pool of over 500,000 public repositories and progressively filters it to 52,550 executable tool candidates under license- and quality-aware criteria. Deploy-Master transforms heterogeneous open-source repositories into runnable, containerized capabilities grounded in execution rather than documentation claims. In a single day, we performed 52,550 build attempts and constructed reproducible runtime environments for 50,112 scientific tools. Each successful tool is validated by a minimal executable command and registered in SciencePedia for search and reuse, enabling direct human use and optional agent-based invocation. Beyond delivering runnable tools, we report a deployment trace at the scale of 50,000 tools, characterizing throughput, cost profiles, failure surfaces, and specification uncertainty that become visible only at scale. These results explain why scientific software remains difficult to operationalize and motivate shared, observable execution substrates as a foundation for scalable AI4S and agentic science.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Wang, Yi and Huang, Zhenting and Ding, Zhaohan and Liao, Ruoxue and Huang, Yuan and Liu, Xinzijian and Xie, Jiajun and Chen, Siheng and Zhang, Linfeng},
	month = jan,
	year = {2026},
	note = {arXiv:2601.03513 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
}

@misc{yang_toward_2026,
	title = {Toward {Efficient} {Agents}: {Memory}, {Tool} learning, and {Planning}},
	shorttitle = {Toward {Efficient} {Agents}},
	url = {http://arxiv.org/abs/2601.14192},
	doi = {10.48550/arXiv.2601.14192},
	abstract = {Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Yang, Xiaofang and Li, Lijun and Zhou, Heng and Zhu, Tong and Qu, Xiaoye and Fan, Yuchen and Wei, Qianshan and Ye, Rui and Kang, Li and Qin, Yiran and Kou, Zhiqiang and Liu, Daizong and Li, Qi and Ding, Ning and Chen, Siheng and Shao, Jing},
	month = jan,
	year = {2026},
	note = {arXiv:2601.14192 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{kanakis_ai_nodate,
	title = {{AI} {Agent} for {User} {Data} {Analysis} in {Quality} {Assurance} for {Mobile} {Applications}},
	abstract = {Writing SQL to extract usage data is a strenuous task that requires extensive knowledge of logging practices and user interactions within an application. For engineers and developers who write SQL only intermittently, the resulting context switching and re-familiarization can consume valuable work hours. To address this challenge, an internal AI agent was developed to transform natural language into project speciﬁc SQL queries for BigQuery databases, reducing or eliminating the need for manual query writing. This automation allows engineers to focus on analyzing user interactions and deriving insights. By analyzing the application source code and interviewing engineers of varying expertise, obstacles to eﬃcient data-driven decision-making were identiﬁed. The complex table structure in BigQuery and the lack of detailed documentation on logging behavior were two major obstacles to eﬃcient query writing that could beneﬁt from AI assistance. The result is a prototype of an AI agent, using the AutoGen framework, available through both terminal and visual interfaces that can be conﬁgured for different teams and projects, facilitating easier and faster data analysis. The agent incorporates domain- and project speciﬁc knowledge of what is logged and why it is logged through the use of descriptors in a custom tool to view the database schema. Testing showed that six out of eight existing queries written by experienced engineers could be reproduced using natural language prompts in less than ﬁve attempts, with an average execution time of just over half a minute from submission to data retrieval. These results highlight the potential of using AI assisted query generation in quality assurance work for the development of mobile applications.},
	language = {en},
	author = {Kanakis, André and Engvall, Frida},
}

@misc{lumer_dont_2026,
	title = {Don't {Break} the {Cache}: {An} {Evaluation} of {Prompt} {Caching} for {Long}-{Horizon} {Agentic} {Tasks}},
	shorttitle = {Don't {Break} the {Cache}},
	url = {http://arxiv.org/abs/2601.06007},
	doi = {10.48550/arXiv.2601.06007},
	abstract = {Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80\% and improves time to first token by 13-31\% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Lumer, Elias and Nizar, Faheem and Jangiti, Akshaya and Frank, Kevin and Gulati, Anmol and Phadate, Mandar and Subbiah, Vamse Kumar},
	month = jan,
	year = {2026},
	note = {arXiv:2601.06007 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@book{jim_dowling_building_2025,
	title = {Building {Machine} {Learning} {Systems} with a {Feature} {Store}},
	language = {eng},
	publisher = {O'Reilly Media, Inc},
	author = {{Jim Dowling}},
	year = {2025},
	note = {OCLC: 1425188410},
}

@misc{chinthareddy_enterprise_2026,
	title = {Enterprise {Identity} {Integration} for {AI}-{Assisted} {Developer} {Services}: {Architecture}, {Implementation}, and {Case} {Study}},
	shorttitle = {Enterprise {Identity} {Integration} for {AI}-{Assisted} {Developer} {Services}},
	url = {http://arxiv.org/abs/2601.02698},
	doi = {10.48550/arXiv.2601.02698},
	abstract = {AI-assisted developer services are increasingly embedded in modern IDEs, yet enterprises must ensure these tools operate within existing identity, access control, and governance requirements. The Model Context Protocol (MCP) enables AI assistants to retrieve structured internal context, but its specification provides only a minimal authorization model and lacks guidance on integrating enterprise SSO. This article presents a practical architecture that incorporates OAuth 2.0 and OpenID Connect (OIDC) into MCP-enabled developer environments. It describes how IDE extensions obtain and present tokens, how MCP servers validate them through an identity provider, and how scopes and claims can enforce least-privilege access. A prototype implementation using Visual Studio Code, a Python-based MCP server, and an OIDC-compliant IdP demonstrates feasibility. A case study evaluates authentication latency, token-validation overhead, operational considerations, and AI-specific risks. The approach provides a deployable pattern for organizations adopting AI-assisted developer tools while maintaining identity assurance and auditability.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Chinthareddy, Manideep Reddy},
	month = jan,
	year = {2026},
	note = {arXiv:2601.02698 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
}

@misc{kulkarni_optimizing_2026,
	title = {Optimizing {FaaS} {Platforms} for {MCP}-enabled {Agentic} {Workflows}},
	url = {http://arxiv.org/abs/2601.14735},
	doi = {10.48550/arXiv.2601.14735},
	abstract = {Agentic workflows that use autonomous AI Agents powered by Large Language Models (LLMs) and Model Context Protocol (MCP) servers is rapidly rising. This introduces challenges in scalable cloud deployment and state management. Traditional hosting on Virtual Machines (VMs) is resource-intensive and lacks elasticity. Functions-as-a-Service (FaaS) platforms offer modularity, autoscaling and cost efficiency but are inherently stateless. In this paper, we present the FAME, a FaaS-based architecture for orchestrating MCP-enabled agentic workflows. FAME decomposes agentic patterns such as ReAct into composable agents: Planner, Actor and Evaluator, that are each a FaaS function built using LangGraph and are orchestrated as a FaaS workflow. This enables modular composition as AWS Step Functions and avoids function timeouts seen for monolithic agentic workflows. To address context persistence across user requests in a conversation, FAME automates agent memory persistence and injection using DynamoDB. It also optimizes MCP server deployment through AWS Lambda wrappers, caches tool outputs in S3 and proposes function fusion strategies. We evaluate FAME on two representative applications, on research paper summarization and log analytics, under diverse memory and caching configurations. Results show up to 13x latency reduction, 88\% fewer input tokens and 66\% in cost savings, along with improved workflow completion rates. This demonstrates the viability of serverless platforms for hosting complex, multi-agent AI workflows at scale.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Kulkarni, Varad and Jha, Vaibhav and Reddy, Nikhil and Eswaran, Anand and Jayachandran, Praveen and Simmhan, Yogesh},
	month = jan,
	year = {2026},
	note = {arXiv:2601.14735 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@misc{tonnarelli_data_2026,
	title = {Data {Product} {MCP}: {Chat} with your {Enterprise} {Data}},
	shorttitle = {Data {Product} {MCP}},
	url = {http://arxiv.org/abs/2601.08687},
	doi = {10.48550/arXiv.2601.08687},
	abstract = {Computational data governance aims to make the enforcement of governance policies and legal obligations more efficient and reliable. Recent advances in natural language processing and agentic AI offer ways to improve how organizations share and use data. But many barriers remain. Today's tools require technical skills and multiple roles to discover, request, and query data. Automating data access using enterprise AI agents is limited by the means to discover and autonomously access distributed data. Current solutions either compromise governance or break agentic workflows through manual approvals. To close this gap, we introduce Data Product MCP integrated in a data product marketplace. This data marketplace, already in use at large enterprises, enables AI agents to find, request, and query enterprise data products while enforcing data contracts in real time without lowering governance standards. The system is built on the Model Context Protocol (MCP) and links the AI-driven marketplace with cloud platforms such as Snowflake, Databricks, and Google Cloud Platform. It supports semantic discovery of data products based on business context, automates access control by validating generated queries against approved business purposes using AI-driven checks, and enforces contracts in real time by blocking unauthorized queries before they run. We assessed the system with feedback from \$n=16\$ experts in data governance. Our qualitative evaluation demonstrates effectiveness through enterprise scenarios such as customer analytics. The findings suggest that Data Product MCP reduces the technical burden for data analysis without weakening governance, filling a key gap in enterprise AI adoption.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Tonnarelli, Marco and Scaramuzza, Filippo and Harrer, Simon and Dietz, Linus W.},
	month = jan,
	year = {2026},
	note = {arXiv:2601.08687 [cs]},
	keywords = {Computer Science - Emerging Technologies},
}

@misc{jayanti_enhancing_2026,
	title = {Enhancing {Model} {Context} {Protocol} ({MCP}) with {Context}-{Aware} {Server} {Collaboration}},
	url = {http://arxiv.org/abs/2601.11595},
	doi = {10.48550/arXiv.2601.11595},
	abstract = {The Model Context Protocol (MCP) (MCP Community, 2025) has emerged as a widely used framework for enabling LLM-based agents to communicate with external tools and services. The original MCP implementation (Anthropic, 2024) relies on a Large Language Model (LLM) to decompose tasks and issue instructions to servers. In particular, the agents, models, and servers are stateless and do not have access to a global context. However, in tasks involving LLM-driven coordination, it is natural that a Shared Context Store (SCS) could improve the efficiency and coherence of multi-agent workflows by reducing redundancy and enabling knowledge transfer between servers. Thus, in this work, we design and assess the performance of a Context-Aware MCP (CA-MCP) that offloads execution logic to specialized MCP servers that read from and write to a shared context memory, allowing them to coordinate more autonomously in real time. In this design, context management serves as the central mechanism that maintains continuity across task executions by tracking intermediate states and shared variables, thereby enabling persistent collaboration among agents without repeated prompting. We present experiments showing that the CA-MCP can outperform the traditional MCP by reducing the number of LLM calls required for complex tasks and decreasing the frequency of response failures when task conditions are not satisfied. In particular, we conducted experiments on the TravelPlanner (Yang et al., 2024) and REALM-Bench (Geng \& Chang, 2025) benchmark datasets and observed statistically significant results indicating the potential advantages of incorporating a shared context store via CA-MCP in LLM-driven multi-agent systems.},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Jayanti, Meenakshi Amulya and Han, X. Y.},
	month = jan,
	year = {2026},
	note = {arXiv:2601.11595 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{noauthor_oppieaitoolsfilter_2026,
	title = {{OppieAI}/{ToolsFilter}},
	url = {https://github.com/OppieAI/ToolsFilter},
	abstract = {Fetch only relevant tools for the current conversation and save cost while increasing the precision of your LLM Response},
	urldate = {2026-01-29},
	publisher = {Oppie AI},
	month = jan,
	year = {2026},
	note = {original-date: 2025-08-17T04:47:38Z},
	keywords = {genai-chatbot, genai-guardrails, genai-usecase, generative-ai, llm-inference, mcp-server, mcp-servers, mcp-tools},
}

@misc{rayarao_bridging_2025,
	title = {Bridging {AI} and {External} {Systems}: {A} {Comprehensive} {Analysis} of the {Model} {Context} {Protocol} ({MCP})},
	shorttitle = {Bridging {AI} and {External} {Systems}},
	url = {https://www.authorea.com/users/868916/articles/1326172-bridging-ai-and-external-systems-a-comprehensive-analysis-of-the-model-context-protocol-mcp?commit=21b44cfc514e00fe6a7b87a64098fe0eb3b96d30},
	doi = {10.22541/au.175555093.31837381/v1},
	urldate = {2026-01-29},
	publisher = {Preprints},
	author = {Rayarao, Surya Rao and Donikena, Naga},
	month = aug,
	year = {2025},
}

@misc{lei_spider_2025,
	title = {Spider 2.0: {Evaluating} {Language} {Models} on {Real}-{World} {Enterprise} {Text}-to-{SQL} {Workflows}},
	shorttitle = {Spider 2.0},
	url = {http://arxiv.org/abs/2411.07763},
	doi = {10.48550/arXiv.2411.07763},
	abstract = {Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising 632 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3\% of the tasks, compared with 91.2\% on Spider 1.0 and 73.0\% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation — especially in prior text-to-SQL benchmarks — they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Our code, baseline models, and data are available at spider2-sql.github.io.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Lei, Fangyu and Chen, Jixuan and Ye, Yuxiao and Cao, Ruisheng and Shin, Dongchan and Su, Hongjin and Suo, Zhaoqing and Gao, Hongcheng and Hu, Wenjing and Yin, Pengcheng and Zhong, Victor and Xiong, Caiming and Sun, Ruoxi and Liu, Qian and Wang, Sida and Yu, Tao},
	month = mar,
	year = {2025},
	note = {arXiv:2411.07763 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases},
}

@misc{wang_tool-assisted_2024,
	title = {Tool-{Assisted} {Agent} on {SQL} {Inspection} and {Refinement} in {Real}-{World} {Scenarios}},
	url = {http://arxiv.org/abs/2408.16991},
	doi = {10.48550/arXiv.2408.16991},
	abstract = {Recent Text-to-SQL methods leverage large language models (LLMs) by incorporating feedback from the database management system. While these methods effectively address execution errors in SQL queries, they struggle with database mismatches—errors that do not trigger execution exceptions. Database mismatches include issues such as condition mismatches and stricter constraint mismatches, both of which are more prevalent in real-world scenarios. To address these challenges, we propose a tool-assisted agent framework for SQL inspection and refinement, equipping the LLM-based agent with two specialized tools: a retriever and a detector, designed to diagnose and correct SQL queries with database mismatches. These tools enhance the capability of LLMs to handle real-world queries more effectively. We also introduce Spider-Mismatch, a new dataset specifically constructed to reflect the condition mismatch problems encountered in realworld scenarios. Experimental results demonstrate that our method achieves the highest performance on the averaged results of the Spider and Spider-Realistic datasets in few-shot settings, and it significantly outperforms baseline methods on the more realistic dataset, Spider-Mismatch.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Wang, Zhongyuan and Zhang, Richong and Nie, Zhijie and Kim, Jaein},
	month = aug,
	year = {2024},
	note = {arXiv:2408.16991 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{xu_everything_2025,
	title = {Everything is {Context}: {Agentic} {File} {System} {Abstraction} for {Context} {Engineering}},
	shorttitle = {Everything is {Context}},
	url = {http://arxiv.org/abs/2512.05470},
	doi = {10.48550/arXiv.2512.05470},
	abstract = {Generative AI (GenAI) has reshaped software system design by introducing foundation models as pre-trained subsystems that redefine architectures and operations. The emerging challenge is no longer model fine-tuning but context engineering-how systems capture, structure, and govern external knowledge, memory, tools, and human input to enable trustworthy reasoning. Existing practices such as prompt engineering, retrieval-augmented generation (RAG), and tool integration remain fragmented, producing transient artefacts that limit traceability and accountability. This paper proposes a file-system abstraction for context engineering, inspired by the Unix notion that 'everything is a file'. The abstraction offers a persistent, governed infrastructure for managing heterogeneous context artefacts through uniform mounting, metadata, and access control. Implemented within the open-source AIGNE framework, the architecture realises a verifiable context-engineering pipeline, comprising the Context Constructor, Loader, and Evaluator, that assembles, delivers, and validates context under token constraints. As GenAI becomes an active collaborator in decision support, humans play a central role as curators, verifiers, and co-reasoners. The proposed architecture establishes a reusable foundation for accountable and human-centred AI co-work, demonstrated through two exemplars: an agent with memory and an MCP-based GitHub assistant. The implementation within the AIGNE framework demonstrates how the architecture can be operationalised in developer and industrial settings, supporting verifiable, maintainable, and industry-ready GenAI systems.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Xu, Xiwei and Mao, Robert and Bai, Quan and Gu, Xuewu and Li, Yechao and Zhu, Liming},
	month = dec,
	year = {2025},
	note = {arXiv:2512.05470 [cs]},
	keywords = {Computer Science - Software Engineering},
}

@misc{hou_model_2025,
	title = {Model {Context} {Protocol} ({MCP}): {Landscape}, {Security} {Threats}, and {Future} {Research} {Directions}},
	shorttitle = {Model {Context} {Protocol} ({MCP})},
	url = {http://arxiv.org/abs/2503.23278},
	doi = {10.48550/arXiv.2503.23278},
	abstract = {XINYI HOU, Huazhong University of Science and Technology, China YANJIE ZHAO, Huazhong University of Science and Technology, China SHENAO WANG, Huazhong University of Science and Technology, China HAOYU WANG∗, Huazhong University of Science and Technology, China The Model Context Protocol (MCP) is an emerging open standard that defines a unified, bi-directional communication and dynamic discovery protocol between AI models and external tools or resources, aiming to enhance interoperability and reduce fragmentation across diverse systems. This paper presents a systematic study of MCP from both architectural and security perspectives. We first define the full lifecycle of an MCP server, comprising four phases (creation, deployment, operation, and maintenance), further decomposed into 16 key activities that capture its functional evolution. Building on this lifecycle analysis, we construct a comprehensive threat taxonomy that categorizes security and privacy risks across four major attacker types: malicious developers, external attackers, malicious users, and security flaws, encompassing 16 distinct threat scenarios. To validate these risks, we develop and analyze real-world case studies that demonstrate concrete attack surfaces and vulnerability manifestations within MCP implementations. Based on these findings, the paper proposes a set of fine-grained, actionable security safeguards tailored to each lifecycle phase and threat category, offering practical guidance for secure MCP adoption. We also analyze the current MCP landscape, covering industry adoption, integration patterns, and supporting tools, to identify its technological strengths as well as existing limitations that constrain broader deployment. Finally, we outline future research and development directions aimed at strengthening MCP’s standardization, trust boundaries, and sustainable growth within the evolving ecosystem of tool-augmented AI systems. All collected data and implementation examples are publicly available at https://github.com/security-pride/MCP\_Landscape. CCS Concepts: • General and reference → Surveys and overviews; • Security and privacy → Software and application security; • Computing methodologies → Artificial intelligence.},
	language = {en},
	urldate = {2026-01-29},
	publisher = {arXiv},
	author = {Hou, Xinyi and Zhao, Yanjie and Wang, Shenao and Wang, Haoyu},
	month = oct,
	year = {2025},
	note = {arXiv:2503.23278 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security},
}

@misc{noauthor_obot-platformobot_2026,
	title = {obot-platform/obot},
	copyright = {MIT},
	url = {https://github.com/obot-platform/obot},
	abstract = {Complete MCP Platform -- Hosting, Registry, Gateway, and Chat Client},
	urldate = {2026-01-29},
	publisher = {Obot AI},
	month = jan,
	year = {2026},
	note = {original-date: 2024-09-05T19:50:46Z},
	keywords = {ai, chat, mcp, modelcontextprotocol},
}

@inproceedings{de_la_rua_martinez_hopsworks_2024,
	address = {Santiago AA Chile},
	title = {The {Hopsworks} {Feature} {Store} for {Machine} {Learning}},
	isbn = {979-8-4007-0422-2},
	url = {https://dl.acm.org/doi/10.1145/3626246.3653389},
	doi = {10.1145/3626246.3653389},
	language = {en},
	urldate = {2025-11-01},
	booktitle = {Companion of the 2024 {International} {Conference} on {Management} of {Data}},
	publisher = {ACM},
	author = {De La Rúa Martínez, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronström, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim},
	month = jun,
	year = {2024},
	pages = {135--147},
}

@incollection{iglesias_accelerating_2026,
	address = {Cham},
	title = {Accelerating {AI} {Agent} {Development} {Through} a {Domain}-{Specific} {Programming} {Language}},
	volume = {1742},
	isbn = {978-3-032-12986-4 978-3-032-12987-1},
	url = {https://link.springer.com/10.1007/978-3-032-12987-1_15},
	doi = {10.1007/978-3-032-12987-1_15},
	language = {en},
	urldate = {2026-01-26},
	booktitle = {Information {Systems} for {Intelligent} {Systems}},
	publisher = {Springer Nature Switzerland},
	author = {Vella, Salvatore and Hussain, Fatima and Sharieh, Salah and Zaheer, Murrium and Ferworn, Alex},
	editor = {Iglesias, Andres and Shin, Jungpil and Bhatt, Nityesh and Joshi, Amit},
	year = {2026},
	note = {Series Title: Lecture Notes in Networks and Systems},
	pages = {149--158},
}

@article{vaidhyanathan_agentic_2026,
	title = {Agentic {AI} {Frameworks} {Under} the {Microscope}: {What} {Works}, {What} {Doesn}’t},
	volume = {43},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Agentic {AI} {Frameworks} {Under} the {Microscope}},
	url = {https://ieeexplore.ieee.org/document/11316910/},
	doi = {10.1109/MS.2025.3622209},
	number = {1},
	urldate = {2026-01-26},
	journal = {IEEE Software},
	author = {Vaidhyanathan, Karthik and Taibi, Davide},
	month = jan,
	year = {2026},
	pages = {133--138},
}

@incollection{swaroop_agentic_2026,
	address = {Cham},
	title = {Agentic {AI} {Across} {Technologies}, {Applications}, and {Development} {Domains}: {A} {Systematic} {Literature} {Review}},
	volume = {1603},
	isbn = {978-3-032-03768-8 978-3-032-03769-5},
	shorttitle = {Agentic {AI} {Across} {Technologies}, {Applications}, and {Development} {Domains}},
	url = {https://link.springer.com/10.1007/978-3-032-03769-5_44},
	doi = {10.1007/978-3-032-03769-5_44},
	language = {en},
	urldate = {2026-01-14},
	booktitle = {Proceedings of {Data} {Analytics} and {Management}},
	publisher = {Springer Nature Switzerland},
	author = {Tran, Giang T. C. and Le Dinh, Thang and Le, Tran Duc},
	editor = {Swaroop, Abhishek and Virdee, Bal and Correia, Sérgio Duarte and Polkowski, Zdzislaw},
	year = {2026},
	note = {Series Title: Lecture Notes in Networks and Systems},
	pages = {587--601},
}

@misc{bousetouane_ai_2026,
	title = {{AI} {Agents} {Need} {Memory} {Control} {Over} {More} {Context}},
	url = {http://arxiv.org/abs/2601.11653},
	doi = {10.48550/arXiv.2601.11653},
	abstract = {AI agents are increasingly used in long, multi-turn workflows in both research and enterprise settings. As interactions grow, agent behavior often degrades due to loss of constraint focus, error accumulation, and memory-induced drift. This problem is especially visible in real-world deployments where context evolves, distractions are introduced, and decisions must remain consistent over time. A common practice is to equip agents with persistent memory through transcript replay or retrieval-based mechanisms. While convenient, these approaches introduce unbounded context growth and are vulnerable to noisy recall and memory poisoning, leading to unstable behavior and increased drift. In this work, we introduce the Agent Cognitive Compressor (ACC), a bio-inspired memory controller that replaces transcript replay with a bounded internal state updated online at each turn. ACC separates artifact recall from state commitment, enabling stable conditioning while preventing unverified content from becoming persistent memory. We evaluate ACC using an agent-judge-driven live evaluation framework that measures both task outcomes and memory-driven anomalies across extended interactions. Across scenarios spanning IT operations, cybersecurity response, and healthcare workflows, ACC consistently maintains bounded memory and exhibits more stable multi-turn behavior, with significantly lower hallucination and drift than transcript replay and retrieval-based agents. These results show that cognitive compression provides a practical and effective foundation for reliable memory control in long-horizon AI agents.},
	urldate = {2026-01-26},
	publisher = {arXiv},
	author = {Bousetouane, Fouad},
	month = jan,
	year = {2026},
	note = {arXiv:2601.11653 [q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Quantitative Biology - Neurons and Cognition},
}

@misc{lin_beyond_2025,
	title = {Beyond {Request}-{Response}: {Architecting} {Real}-time {Bidirectional} {Streaming} {Multi}-agent {System}},
	url = {https://developers.googleblog.com/en/beyond-request-response-architecting-real-time-bidirectional-streaming-multi-agent-system/},
	author = {Lin, Hangfei},
	month = jan,
	year = {2025},
}

@inproceedings{venugopal_enterprise_2025,
	address = {Cairo, Egypt},
	title = {Enterprise {AI} {Agents}: {Secure}, {Scalable}, and {Autonomous} {Intelligence} for the {Modern} {Workforce}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3315-4520-8},
	shorttitle = {Enterprise {AI} {Agents}},
	url = {https://ieeexplore.ieee.org/document/11324763/},
	doi = {10.1109/ICCED68324.2025.11324763},
	urldate = {2026-01-26},
	booktitle = {2025 {IEEE} 11th {International} {Conference} on {Computing}, {Engineering} and {Design} ({ICCED})},
	publisher = {IEEE},
	author = {Venugopal, Shanmugaraja Krishnasamy and Lalakiya, Shivam Ashokbhai and Bharathan, Rashmi and Raja, Pradeep},
	month = nov,
	year = {2025},
	pages = {1--6},
}

@misc{zhu_static_2025,
	title = {From {Static} to {Dynamic}: {A} {Streaming} {RAG} {Approach} to {Real}-time {Knowledge} {Base}},
	shorttitle = {From {Static} to {Dynamic}},
	url = {http://arxiv.org/abs/2508.05662},
	doi = {10.48550/arXiv.2508.05662},
	abstract = {Dynamic streams from news feeds, social media, sensor networks, and financial markets challenge static RAG frameworks. Full-scale indices incur high memory costs; periodic rebuilds introduce latency that undermines data freshness; naive sampling sacrifices semantic coverage. We present Streaming RAG, a unified pipeline that combines multi-vector cosine screening, mini-batch clustering, and a counter-based heavy-hitter filter to maintain a compact prototype set. We further prove an approximation bound {\textbackslash}\$E{\textbackslash}[R(K{\textbackslash}\_t)] {\textbackslash}ge R{\textasciicircum}{\textbackslash}* - L {\textbackslash}Delta{\textbackslash}\$ linking retrieval quality to clustering variance. An incremental index upsert mechanism refreshes prototypes without interrupting queries. Experiments on eight real-time streams show statistically significant gains in Recall{\textbackslash}@10 (up to 3 points, p {\textless} 0.01), end-to-end latency below 15 ms, and throughput above 900 documents per second under a 150 MB budget. Hyperparameter sensitivity analysis over cluster count, admission probability, relevance threshold, and counter capacity validates default settings. In open-domain question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L improvements. Streaming RAG establishes a new Pareto frontier for retrieval augmentation.},
	urldate = {2025-11-01},
	publisher = {arXiv},
	author = {Zhu, Yuzhou},
	month = jul,
	year = {2025},
	note = {arXiv:2508.05662 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@book{bock_getting_2001,
	address = {San Diego},
	title = {Getting it right: {R}\&{D} methods for science and engineering},
	isbn = {978-0-12-108852-1},
	shorttitle = {Getting it right},
	language = {eng},
	publisher = {Academic Press},
	author = {Bock, Peter and Scheibe, Bettina},
	year = {2001},
}

@incollection{lanka_scaling_2026,
	address = {Cham},
	title = {Scaling {AI} {Agents} in the {Enterprises}: {A} {Strategic} {Framework}, {Architecture}, {Governance}, and {KPIs}},
	volume = {1755},
	isbn = {978-3-032-13176-8 978-3-032-13177-5},
	shorttitle = {Scaling {AI} {Agents} in the {Enterprises}},
	url = {https://link.springer.com/10.1007/978-3-032-13177-5_14},
	doi = {10.1007/978-3-032-13177-5_14},
	language = {en},
	urldate = {2026-01-26},
	booktitle = {Trends in {Sustainable} {Computing} and {Machine} {Intelligence}},
	publisher = {Springer Nature Switzerland},
	author = {Butte, Vijay Kumar and Butte, Sujata and Suryakanth, Sagar},
	editor = {Lanka, Surekha and Cabezuelo, Antonio Sarasa and Tugui, Alexandru},
	year = {2026},
	note = {Series Title: Lecture Notes in Networks and Systems},
	pages = {174--188},
}

@inproceedings{v_scaling_2025,
	address = {Bangalore, India},
	title = {Scaling {Multi}-{MCP} {AI} {Agents} {Beyond} {Context} {Limits}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {979-8-3315-8894-6},
	url = {https://ieeexplore.ieee.org/document/11294866/},
	doi = {10.1109/CSITSS67709.2025.11294866},
	urldate = {2026-01-26},
	booktitle = {2025 9th {International} {Conference} on {Computational} {System} and {Information} {Technology} for {Sustainable} {Solutions} ({CSITSS})},
	publisher = {IEEE},
	author = {V, Kiran and Prajapati, Vijay Kumar},
	month = nov,
	year = {2025},
	pages = {1--6},
}

@misc{langchain_streaming_2025,
	title = {Streaming},
	url = {https://docs.langchain.com/oss/python/langchain/streaming},
	author = {{Langchain}},
	month = jan,
	year = {2025},
}

@misc{noauthor_tursodatabaseagentfs_2026,
	title = {tursodatabase/agentfs},
	url = {https://github.com/tursodatabase/agentfs},
	abstract = {The filesystem for agents.},
	urldate = {2026-01-13},
	publisher = {Turso Database},
	month = jan,
	year = {2026},
	note = {original-date: 2025-10-24T17:05:06Z},
	keywords = {agents, filesystem, sqlite, turso},
}

@misc{noauthor_logicalclockshopsworks-api_2025,
	title = {logicalclocks/hopsworks-api},
	copyright = {Apache-2.0},
	url = {https://github.com/logicalclocks/hopsworks-api},
	abstract = {Python SDK to interact with the Hopsworks API},
	urldate = {2026-01-13},
	publisher = {Hopsworks},
	month = dec,
	year = {2025},
	note = {original-date: 2021-11-13T07:21:45Z},
}
